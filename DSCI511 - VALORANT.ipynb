{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea9782b3",
   "metadata": {},
   "source": [
    "# DSCI 511: Data Acquisition and Pre-Processing <br> Term Project Phase 2: Valorant comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01540c5",
   "metadata": {},
   "source": [
    "## Group members \n",
    "- Group member \n",
    "    - Name: Amira Bendjama\n",
    "    - Email: ab4745@drexel.edu\n",
    "- Group member \n",
    "    - Name: Nicole Padilla \n",
    "    - Email: np858@drexel.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d500e8f3",
   "metadata": {},
   "source": [
    "# Data collection \n",
    "\n",
    "Initial data was gathered via the YouTube API which allows publicly available YouTube comments to be called by anyone who created an app with their Google account. [GeeksforGeeks](https://www.geeksforgeeks.org/how-to-extract-youtube-comments-using-youtube-api-python/) was used as a reference for the code written to run the API call. The code was modified and resulting data was loaded into a .csv file.\n",
    "There are 22 youtubers selected based on their subscription count. We considered big youtubers the ones that their channels' subscription count surpass 500k, and under 500k to 100k are considered small youtubers. The lower bound 100k for subscription is how much the channel must reach in order to be verified, and since brands look for verified channels, we considered that limit. we collected 30 channel information from each youtuber and retrieved all the comments from the channels which generated 281480 comments.\n",
    "\n",
    "The youtubers information are collected in csv file \"Youtubers.csv\" that contains 3 columns: channel name, subscription count, channel's URL. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f2c8c2",
   "metadata": {},
   "source": [
    "### Youtubers \n",
    "In order to start collecting the comments, we needed a dataset of youtubers. Our selection was based on articles from [Best Valorant Streamers](https://www.esportsbets.com/valorant/streamers/), [Valorant main page on youtube](https://www.youtube.com/channel/UCiMRGE8Sc6oxIGuu_JxFoHg/live), reddit posts about [Valorant favorite youtubers](https://www.reddit.com/r/VALORANT/comments/o29j7i/favourite_valorant_youtuber/), and [Valorant YouTuber to learn the basics ?\n",
    "](https://www.reddit.com/r/VALORANT/comments/vz5mjp/valorant_youtuber_to_learn_the_basics/).\n",
    "\n",
    "__Criteria for picking streamers__: \n",
    "- Only verified channels, with a lower bound of subscription count of 100k, since the latter is how much the channel must reach in order to be eligible to apply for verification, and companies and brands will only consider verified channel to promote their products, in our case games.\n",
    "- Most valorant streamers are based on twitch, so a popular twitch streamers doesn’t qualify as a popular youtuber, so we picked valorant youtubers that upload on their main youtube channel and have a certain subscription count. \n",
    "- The valorant youtubers are split into two categories: Big youtubers above 500k subscription count, and small youtubers are under and above 100k. \n",
    "- Youtbers are english speakers from around the world, so it is not based on location but language.\n",
    "- Youtube channels are mixed between channels with only valorant videos, and channels with variety of other content besides valorant. Mainly to see the comment section through different communities. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "405ab2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def get_channels_names(file_path):\n",
    "    youtubers = pd.read_csv(file_path, sep = \",\", header = 0)\n",
    "    return youtubers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c1064cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_name</th>\n",
       "      <th>sub_count</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shroud</td>\n",
       "      <td>6.81M</td>\n",
       "      <td>https://www.youtube.com/@shroud/videos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sykkuno</td>\n",
       "      <td>2.89M</td>\n",
       "      <td>https://www.youtube.com/@Sykkuno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iiTzTimmy</td>\n",
       "      <td>1.63M</td>\n",
       "      <td>https://www.youtube.com/@iiTzTimmy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TenZ</td>\n",
       "      <td>1.59M</td>\n",
       "      <td>https://www.youtube.com/@TenZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Flights</td>\n",
       "      <td>918K</td>\n",
       "      <td>https://www.youtube.com/@Flightss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Grim</td>\n",
       "      <td>893K</td>\n",
       "      <td>https://www.youtube.com/c/GrimGuy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kaydae</td>\n",
       "      <td>879K</td>\n",
       "      <td>https://www.youtube.com/@Kyedae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fuslie</td>\n",
       "      <td>732K</td>\n",
       "      <td>https://www.youtube.com/@fuslie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tarik</td>\n",
       "      <td>660K</td>\n",
       "      <td>https://www.youtube.com/@tarik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MrLowlander</td>\n",
       "      <td>624K</td>\n",
       "      <td>https://www.youtube.com/@MrLowlander</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>noted</td>\n",
       "      <td>612K</td>\n",
       "      <td>https://www.youtube.com/@noted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Flexinja</td>\n",
       "      <td>474K</td>\n",
       "      <td>https://www.youtube.com/@Flexinja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>QuarterJade</td>\n",
       "      <td>434K</td>\n",
       "      <td>https://www.youtube.com/@QuarterJade/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>xirena</td>\n",
       "      <td>392K</td>\n",
       "      <td>https://www.youtube.com/@xirena</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Hiko</td>\n",
       "      <td>384K</td>\n",
       "      <td>https://www.youtube.com/@Hiko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>red</td>\n",
       "      <td>329K</td>\n",
       "      <td>https://www.youtube.com/@RedValorant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Keeoh</td>\n",
       "      <td>339K</td>\n",
       "      <td>https://www.youtube.com/@Keeoh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Ziptie</td>\n",
       "      <td>251K</td>\n",
       "      <td>https://www.youtube.com/@ZipTie/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>xChocoBars</td>\n",
       "      <td>243K</td>\n",
       "      <td>https://www.youtube.com/@xChocoBars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>vkimm</td>\n",
       "      <td>234K</td>\n",
       "      <td>https://www.youtube.com/@vkimm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Peak</td>\n",
       "      <td>172K</td>\n",
       "      <td>https://www.youtube.com/@GosuPeak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Sydeon</td>\n",
       "      <td>115K</td>\n",
       "      <td>https://www.youtube.com/@Sydeons</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    channel_name sub_count                                     url\n",
       "0         Shroud     6.81M  https://www.youtube.com/@shroud/videos\n",
       "1        Sykkuno     2.89M        https://www.youtube.com/@Sykkuno\n",
       "2     iiTzTimmy      1.63M      https://www.youtube.com/@iiTzTimmy\n",
       "3           TenZ     1.59M           https://www.youtube.com/@TenZ\n",
       "4       Flights       918K       https://www.youtube.com/@Flightss\n",
       "5           Grim     893K        https://www.youtube.com/c/GrimGuy\n",
       "6         Kaydae      879K         https://www.youtube.com/@Kyedae\n",
       "7        fuslie       732K         https://www.youtube.com/@fuslie\n",
       "8          Tarik      660K          https://www.youtube.com/@tarik\n",
       "9   MrLowlander       624K    https://www.youtube.com/@MrLowlander\n",
       "10        noted      612K           https://www.youtube.com/@noted\n",
       "11      Flexinja      474K       https://www.youtube.com/@Flexinja\n",
       "12  QuarterJade       434K   https://www.youtube.com/@QuarterJade/\n",
       "13       xirena      392K          https://www.youtube.com/@xirena\n",
       "14          Hiko      384K           https://www.youtube.com/@Hiko\n",
       "15           red      329K    https://www.youtube.com/@RedValorant\n",
       "16         Keeoh      339K          https://www.youtube.com/@Keeoh\n",
       "17       Ziptie       251K        https://www.youtube.com/@ZipTie/\n",
       "18    xChocoBars     243K      https://www.youtube.com/@xChocoBars\n",
       "19         vkimm      234K          https://www.youtube.com/@vkimm\n",
       "20         Peak       172K       https://www.youtube.com/@GosuPeak\n",
       "21        Sydeon      115K        https://www.youtube.com/@Sydeons"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtubers = get_channels_names(\"data/Youtubers.csv\")\n",
    "youtubers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce051067",
   "metadata": {},
   "source": [
    "## Youtube API \n",
    "In this project, we used Youtube API to retrieve comments, and videos from channels. We mainly used [youtube guide](https://developers.google.com/youtube/v3/getting-started), and other [ressources](https://towardsdatascience.com/how-to-build-your-own-dataset-of-youtube-comments-39a1e57aade). \n",
    "In order to access the API, a project must be created in [Google Developer’s Console](https://console.cloud.google.com/apis/dashboard?project=caramel-logic-370101), where you will have to do two steps: \n",
    "* Enable Youtube API data API v3.\n",
    "* Create API key.\n",
    "\n",
    "__Quota__ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147c1b8b",
   "metadata": {},
   "source": [
    "## Part 1: Retreiving Valorant Youtube comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a74162",
   "metadata": {},
   "source": [
    "### Building Youtube service \n",
    "After setting up the youtube API, we must install libraries for Google API client for python. <br>\n",
    "There is a quota limitation set by google at 10,000 units per day. To tackle this limitation, we used 4 different API keys to be able to retrieve the amount of videos and comments we want.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54afe589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AIzaSyCuPRmg3boEYuCK_IUmX5QthRiVnwOGkFk',\n",
       " 'AIzaSyCBCRhyNZh98DEOWx0UH4QFgAMqbcVJqho',\n",
       " 'AIzaSyAkKs_1ndolibMgBUR94PQi1MJoGGM6mU0',\n",
       " 'AIzaSyAnhWSJGOoFQwdiB5DFnNeMPfrGMUpm04w']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_keys(file_path):\n",
    "    with open('data/keys.txt' , \"r\") as f: \n",
    "        keys = f.read()\n",
    "    keys = keys.split(\"\\n\")\n",
    "    return keys\n",
    "\n",
    "keys = get_keys('data/keys.txt')\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af63f266",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade google-api-python-client\n",
    "from googleapiclient.discovery import build\n",
    "#building youtube service\n",
    "def youtube_build_service(KEY):\n",
    "    \n",
    "    YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "    YOUTUBE_API_VERSION = \"v3\"\n",
    "\n",
    "    return build(YOUTUBE_API_SERVICE_NAME,\n",
    "                 YOUTUBE_API_VERSION,\n",
    "                 developerKey=KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d01a229",
   "metadata": {},
   "source": [
    "Getting keys to build the youtube service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e395f366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#each time call the service pop keys\n",
    "def get_service():\n",
    "    global youtube_service \n",
    "    if keys:\n",
    "        youtube_service  = youtube_build_service(keys.pop())       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f95663",
   "metadata": {},
   "source": [
    "Call the service each time the quota ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fc29b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<googleapiclient.discovery.Resource at 0x21b3e05a7c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#call this function to build the service \n",
    "#and also to switch keys\n",
    "get_service()\n",
    "youtube_service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc951d47",
   "metadata": {},
   "source": [
    "### Channel information \n",
    "Each youtube channel has a unique channel ID, that mostly can be found at the end of the URL. However, some of old URL main channels will have the unique channel ID where others channels will have the name of the channel instead in form of: https://www.youtube.com/@namechannel. To solve issue, BeautifulSoup and requests were used to fecth html page of each channel and getting the unique ID by finding \"externalId\" that has the channel ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a20dbc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def get_channel_id(channel_url):\n",
    "    url =\"\" \n",
    "    #getting json\n",
    "    resp = requests.get(channel_url)\n",
    "    data = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    #finding \"externalId\" that has the channel id no matter what is link structure\n",
    "    data_s = str(data)\n",
    "    \n",
    "    search_url = re.search('\"externalId\":',data_s)\n",
    "    start, end = search_url.span()\n",
    "    #finding the url after the id, using index\n",
    "    for i in range(end , end+100):\n",
    "        if data_s[i] == \",\":\n",
    "            break\n",
    "        url += data_s[i]\n",
    "    url = url.split('\"')[1]\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742d128e",
   "metadata": {},
   "source": [
    "Using API call, to get channels information, specifiying statistics, snippets, contentDetails.Also, Quota consumption is 1 quota for each youtube list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc2f649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_details(youtube, **kwargs):\n",
    "    return youtube.channels().list(\n",
    "        part=\"statistics,snippet,contentDetails\",\n",
    "        **kwargs\n",
    "    ).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2582402",
   "metadata": {},
   "source": [
    "Fetching each channel detail by providing the URL, then extracting the information needed from the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c55e0e31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_channels_details_info(youtubers, youtube_service):\n",
    "    dict_youtubers = {}\n",
    "    l_youtubers = []\n",
    "    for index in range(len(youtubers[\"url\"])):\n",
    "        # get the channel ID from the URL\n",
    "        channel_id= get_channel_id(youtubers[\"url\"].iloc[index])\n",
    "        # get the channel details\n",
    "        response = get_channel_details(youtube_service, id=channel_id)\n",
    "        snippet = response[\"items\"][0][\"snippet\"]\n",
    "        statistics = response[\"items\"][0][\"statistics\"]\n",
    "        dict_youtubers = {\n",
    "            \"channel_id\":channel_id,\n",
    "            \"channel_title\" : snippet[\"title\"],\n",
    "            \"channel_subscriber_count\" : statistics[\"subscriberCount\"],\n",
    "            \"channel_video_count\" : statistics[\"videoCount\"],\n",
    "            \"channel_view_count\"  : statistics[\"viewCount\"] \n",
    "        }\n",
    "        l_youtubers.append(dict_youtubers)\n",
    "        \n",
    "    return l_youtubers\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daf4b9d",
   "metadata": {},
   "source": [
    "Saving/loading channels information into/from \"./data/channels.csv\" after fetching 5 columns:\n",
    "* \"channel_id\"\n",
    "* \"channel_title\"\n",
    "* \"channel_subscriber_count\"\n",
    "* \"channel_video_count\"\n",
    "* \"channel_view_count\" </br>\n",
    "\n",
    "All the information will be presented as dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fbb76a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_id</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>channel_subscriber_count</th>\n",
       "      <th>channel_video_count</th>\n",
       "      <th>channel_view_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UCoz3Kpu5lv-ALhR4h9bDvcw</td>\n",
       "      <td>Shroud</td>\n",
       "      <td>6810000</td>\n",
       "      <td>1428</td>\n",
       "      <td>1007951954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UCRAEUAmW9kletIzOxhpLRFw</td>\n",
       "      <td>Sykkuno</td>\n",
       "      <td>2890000</td>\n",
       "      <td>641</td>\n",
       "      <td>371445453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UC5v2QgY2D5tlu8uws23MG4Q</td>\n",
       "      <td>iiTzTimmy</td>\n",
       "      <td>1630000</td>\n",
       "      <td>745</td>\n",
       "      <td>270690657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UCckPYr9b_iVucz8ID1Q67sw</td>\n",
       "      <td>TenZ</td>\n",
       "      <td>1590000</td>\n",
       "      <td>251</td>\n",
       "      <td>156859008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UCIfAlCwj-ZPZq5fqjpYDX3w</td>\n",
       "      <td>Flights</td>\n",
       "      <td>918000</td>\n",
       "      <td>56</td>\n",
       "      <td>96612905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UCWphjEePrzIrRA5mwcOt_4Q</td>\n",
       "      <td>Grim</td>\n",
       "      <td>893000</td>\n",
       "      <td>226</td>\n",
       "      <td>107176110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UCxjdy5n9BxX_6RTL8Dt_7pg</td>\n",
       "      <td>Kyedae</td>\n",
       "      <td>880000</td>\n",
       "      <td>81</td>\n",
       "      <td>52712496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>UCujyjxsq5FZNVnQro51zKSQ</td>\n",
       "      <td>fuslie</td>\n",
       "      <td>735000</td>\n",
       "      <td>785</td>\n",
       "      <td>120281865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>UCTbtlMEiBfs0zZLQyJzR0Uw</td>\n",
       "      <td>tarik</td>\n",
       "      <td>661000</td>\n",
       "      <td>1269</td>\n",
       "      <td>160465751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>UCgtbMb3djcXKj6CHerHwZ-A</td>\n",
       "      <td>MrLowlander</td>\n",
       "      <td>625000</td>\n",
       "      <td>366</td>\n",
       "      <td>182226931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>UCQ4dS_JStXcK3A30isduBbg</td>\n",
       "      <td>noted</td>\n",
       "      <td>613000</td>\n",
       "      <td>393</td>\n",
       "      <td>73104319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>UCqoJxH5s6xAiJ6QyevmuG7Q</td>\n",
       "      <td>Flexinja</td>\n",
       "      <td>474000</td>\n",
       "      <td>620</td>\n",
       "      <td>64033399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>UC_wSuaxwUYsJOBZDWwHIQZg</td>\n",
       "      <td>QuarterJade</td>\n",
       "      <td>435000</td>\n",
       "      <td>383</td>\n",
       "      <td>71545194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>UCdSad9tpJS8V8rL-4iuRuYw</td>\n",
       "      <td>xirena</td>\n",
       "      <td>393000</td>\n",
       "      <td>57</td>\n",
       "      <td>31534877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>UCRN1XC7PnnTL5R_GbYOMCZg</td>\n",
       "      <td>Hiko</td>\n",
       "      <td>384000</td>\n",
       "      <td>394</td>\n",
       "      <td>55728176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>UCFJ1pr8iwWPeQjmeHnPhqvA</td>\n",
       "      <td>Red</td>\n",
       "      <td>329000</td>\n",
       "      <td>294</td>\n",
       "      <td>53109590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>UCeIcwvxA_e5Dvrqg3rsuN1w</td>\n",
       "      <td>Keeoh</td>\n",
       "      <td>339000</td>\n",
       "      <td>384</td>\n",
       "      <td>69056269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>UCQ8VQZoYPeXF_q0E19UDGYQ</td>\n",
       "      <td>Ziptie</td>\n",
       "      <td>253000</td>\n",
       "      <td>238</td>\n",
       "      <td>81174305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>UCdH7fwkQ5RGVAMIAN2ufm4Q</td>\n",
       "      <td>xChocoBars</td>\n",
       "      <td>243000</td>\n",
       "      <td>570</td>\n",
       "      <td>18828574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>UCCBJqqk5h2hh8_WDGzrkRCQ</td>\n",
       "      <td>vkimm</td>\n",
       "      <td>235000</td>\n",
       "      <td>163</td>\n",
       "      <td>34621869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>UC-_1FH52GIOFGu4a8PzwRzQ</td>\n",
       "      <td>Peak</td>\n",
       "      <td>172000</td>\n",
       "      <td>486</td>\n",
       "      <td>23956005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>UCtTWOND3uyl4tVc_FarDmpw</td>\n",
       "      <td>Sydeon</td>\n",
       "      <td>115000</td>\n",
       "      <td>162</td>\n",
       "      <td>5914707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  channel_id channel_title  channel_subscriber_count  \\\n",
       "0   UCoz3Kpu5lv-ALhR4h9bDvcw        Shroud                   6810000   \n",
       "1   UCRAEUAmW9kletIzOxhpLRFw       Sykkuno                   2890000   \n",
       "2   UC5v2QgY2D5tlu8uws23MG4Q     iiTzTimmy                   1630000   \n",
       "3   UCckPYr9b_iVucz8ID1Q67sw          TenZ                   1590000   \n",
       "4   UCIfAlCwj-ZPZq5fqjpYDX3w       Flights                    918000   \n",
       "5   UCWphjEePrzIrRA5mwcOt_4Q          Grim                    893000   \n",
       "6   UCxjdy5n9BxX_6RTL8Dt_7pg        Kyedae                    880000   \n",
       "7   UCujyjxsq5FZNVnQro51zKSQ        fuslie                    735000   \n",
       "8   UCTbtlMEiBfs0zZLQyJzR0Uw         tarik                    661000   \n",
       "9   UCgtbMb3djcXKj6CHerHwZ-A   MrLowlander                    625000   \n",
       "10  UCQ4dS_JStXcK3A30isduBbg         noted                    613000   \n",
       "11  UCqoJxH5s6xAiJ6QyevmuG7Q      Flexinja                    474000   \n",
       "12  UC_wSuaxwUYsJOBZDWwHIQZg   QuarterJade                    435000   \n",
       "13  UCdSad9tpJS8V8rL-4iuRuYw        xirena                    393000   \n",
       "14  UCRN1XC7PnnTL5R_GbYOMCZg          Hiko                    384000   \n",
       "15  UCFJ1pr8iwWPeQjmeHnPhqvA           Red                    329000   \n",
       "16  UCeIcwvxA_e5Dvrqg3rsuN1w         Keeoh                    339000   \n",
       "17  UCQ8VQZoYPeXF_q0E19UDGYQ        Ziptie                    253000   \n",
       "18  UCdH7fwkQ5RGVAMIAN2ufm4Q    xChocoBars                    243000   \n",
       "19  UCCBJqqk5h2hh8_WDGzrkRCQ         vkimm                    235000   \n",
       "20  UC-_1FH52GIOFGu4a8PzwRzQ          Peak                    172000   \n",
       "21  UCtTWOND3uyl4tVc_FarDmpw        Sydeon                    115000   \n",
       "\n",
       "    channel_video_count  channel_view_count  \n",
       "0                  1428          1007951954  \n",
       "1                   641           371445453  \n",
       "2                   745           270690657  \n",
       "3                   251           156859008  \n",
       "4                    56            96612905  \n",
       "5                   226           107176110  \n",
       "6                    81            52712496  \n",
       "7                   785           120281865  \n",
       "8                  1269           160465751  \n",
       "9                   366           182226931  \n",
       "10                  393            73104319  \n",
       "11                  620            64033399  \n",
       "12                  383            71545194  \n",
       "13                   57            31534877  \n",
       "14                  394            55728176  \n",
       "15                  294            53109590  \n",
       "16                  384            69056269  \n",
       "17                  238            81174305  \n",
       "18                  570            18828574  \n",
       "19                  163            34621869  \n",
       "20                  486            23956005  \n",
       "21                  162             5914707  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists(\"data/channels.csv\"):\n",
    "    # load any pre-existing data\n",
    "    df = pd.read_csv('data/channels.csv')\n",
    "else:\n",
    "    channels_info = get_channels_details_info(youtubers, youtube_service)\n",
    "    df = pd.DataFrame(channels_info)\n",
    "    #save to csv file\n",
    "    df.to_csv('data/channels.csv', index=False)\n",
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ac8b9",
   "metadata": {},
   "source": [
    "### Extracting videos from each channel\n",
    "Manually picking up valorant videos for each channel isn't convenient. In addition, most videos won't have valorant in the title.To address this issue, we used the [search()](https://developers.google.com/youtube/v3/docs/search/list) offered by youtube API, where it has \"q\" paramter that specifies the query term to search for.<br>\n",
    "We were able to extract roughly 462 videos,by fetching 21 videos for each youtuber."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20d2b81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_videos(youtube, **kwargs):\n",
    "    return youtube.search().list(\n",
    "        **kwargs\n",
    "    ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b500a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_details(youtube, **kwargs):\n",
    "    return youtube.videos().list(\n",
    "        part=\"snippet,contentDetails,statistics\",\n",
    "        **kwargs\n",
    "    ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c9339eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def video_infos(video_response):\n",
    "     \n",
    "    items = video_response.get(\"items\")[0]\n",
    "    # get the snippet, statistics & content details from the video response\n",
    "    snippet         = items[\"snippet\"]\n",
    "    statistics      = items[\"statistics\"]\n",
    "    content_details = items[\"contentDetails\"]\n",
    "    # get infos from the snippet\n",
    "    channel_title = snippet[\"channelTitle\"]\n",
    "    channel_id = snippet[\"channelId\"]\n",
    "    title         = snippet[\"title\"]\n",
    "    #parse publish time\n",
    "    publish_time  = datetime.datetime.strptime(snippet[\"publishedAt\"],\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    \n",
    "    # get stats infos\n",
    "    comment_count = statistics[\"commentCount\"]\n",
    "    like_count    = statistics[\"likeCount\"]\n",
    "    view_count    = statistics[\"viewCount\"]\n",
    "    # get duration from content details\n",
    "    duration = content_details[\"duration\"]\n",
    "    \n",
    "    # duration in the form of something like 'PT5H50M15S'\n",
    "    # parsing it to be something like '5:50:15'\n",
    "    parsed_duration = re.search(f\"PT(\\d+H)?(\\d+M)?(\\d+S)?\", duration).groups()\n",
    "    duration_str = \"\"\n",
    "    for d in parsed_duration:\n",
    "        if d:\n",
    "            duration_str += f\"{d[:-1]}:\"\n",
    "    duration_str = duration_str.strip(\":\")\n",
    "    \n",
    "    dict_video_info = {\n",
    "        \"Title\": title,\n",
    "        \"Channel Title\": channel_title,\n",
    "        \"Channel ID\": channel_id,\n",
    "        \"Publish time\": publish_time,\n",
    "        \"Duration\": duration_str,\n",
    "        \"Number of comments\": comment_count,\n",
    "        \"Number of likes\": like_count,\n",
    "        \"Number of views\": view_count\n",
    "        \n",
    "    }\n",
    "    \n",
    "    return dict_video_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3da335",
   "metadata": {},
   "source": [
    "The main issue we faced throughout the project is the quota limitation. To handle that, we used try/except to handle the HttpError generated from reaching the limits. When our limits reached for a single key, it is switched to another key and we build the youtube service again. Also, we made sure to undestand the quota consumption for each function using [YouTube Data API v3 - Quota Calculator](https://developers.google.com/youtube/v3/determine_quota_cost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9eddccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_videos_from_channel(youtube_service, channel_id, videos_limit = 5):\n",
    "    \n",
    "    # counting number of videos grabbed\n",
    "    n_videos = 0\n",
    "    next_page_token = None\n",
    "    list_videos = []\n",
    "    \n",
    "\n",
    "    while n_videos < videos_limit:\n",
    "        #paramters to select the videos\n",
    "        #only valorant related videos\n",
    "        params = {\n",
    "            'part': 'snippet',\n",
    "            'q': 'valorant',\n",
    "            'channelId': channel_id,\n",
    "            'type': 'video',\n",
    "        }\n",
    "        \n",
    "        if next_page_token:\n",
    "            params['pageToken'] = next_page_token\n",
    "        \n",
    "        try:\n",
    "            #getting channel videos based on parameters\n",
    "            res = get_channel_videos(youtube_service, **params)\n",
    "            #getting items\n",
    "            channel_videos = res.get(\"items\")\n",
    "\n",
    "            for video in channel_videos:\n",
    "                if n_videos == videos_limit:\n",
    "                    break\n",
    "\n",
    "                \n",
    "                video_id = video[\"id\"][\"videoId\"]\n",
    "                # easily construct video URL by its ID\n",
    "                video_url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "\n",
    "                video_response = get_video_details(youtube_service, id=video_id)\n",
    "\n",
    "                # get video details in dictionary\n",
    "                dictionary_video = video_infos(video_response)\n",
    "                dictionary_video[\"video_id\"] = video_id\n",
    "                dictionary_video[\"url\"] = video_url \n",
    "                #changed just location\n",
    "                n_videos += 1\n",
    "\n",
    "                list_videos.append(dictionary_video)\n",
    "\n",
    "            # if there is a next page, then add it to our parameters\n",
    "            # to proceed to the next page\n",
    "            if \"nextPageToken\" in res:\n",
    "                next_page_token = res[\"nextPageToken\"]\n",
    "            \n",
    "           \n",
    "            \n",
    "        #catch the quota exception and switch keys\n",
    "        except Exception as e:\n",
    "            if keys:\n",
    "                #switch key and build service\n",
    "                print(\"switch keys\", keys)\n",
    "                get_service()\n",
    "                continue\n",
    "            else: \n",
    "                #in case of not having keys\n",
    "                print(\"break\", keys)\n",
    "                return list_videos\n",
    "\n",
    "        \n",
    "    return list_videos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081ecb4a",
   "metadata": {},
   "source": [
    "Saving/loading channels information into/from csv file after fetching 10 columns: __Title,\tChannel Title, Channel ID, Publish time,\tDuration,\tNumber of comments,\tNumber of likes,\tNumber of views,\tvideo_id,\turl.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bba8ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Channel Title</th>\n",
       "      <th>Channel ID</th>\n",
       "      <th>Publish time</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Number of comments</th>\n",
       "      <th>Number of likes</th>\n",
       "      <th>Number of views</th>\n",
       "      <th>video_id</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'M BACK</td>\n",
       "      <td>Shroud</td>\n",
       "      <td>UCoz3Kpu5lv-ALhR4h9bDvcw</td>\n",
       "      <td>2022-11-29 22:16:54</td>\n",
       "      <td>10:14</td>\n",
       "      <td>418</td>\n",
       "      <td>10111</td>\n",
       "      <td>255065</td>\n",
       "      <td>jDW6uIbZHO0</td>\n",
       "      <td>https://www.youtube.com/watch?v=jDW6uIbZHO0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9 MINUTES OF SHROUD DESTROYING TRASH TALKERS I...</td>\n",
       "      <td>Shroud</td>\n",
       "      <td>UCoz3Kpu5lv-ALhR4h9bDvcw</td>\n",
       "      <td>2022-10-01 13:01:41</td>\n",
       "      <td>9:39</td>\n",
       "      <td>685</td>\n",
       "      <td>22175</td>\n",
       "      <td>561655</td>\n",
       "      <td>DTuS6Bki9kI</td>\n",
       "      <td>https://www.youtube.com/watch?v=DTuS6Bki9kI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have an announcement</td>\n",
       "      <td>Shroud</td>\n",
       "      <td>UCoz3Kpu5lv-ALhR4h9bDvcw</td>\n",
       "      <td>2022-11-23 22:19:00</td>\n",
       "      <td>10:14</td>\n",
       "      <td>1542</td>\n",
       "      <td>34935</td>\n",
       "      <td>1139897</td>\n",
       "      <td>7OWtsq-1V2Y</td>\n",
       "      <td>https://www.youtube.com/watch?v=7OWtsq-1V2Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CAN 4 VALORANT PROS BEAT 5 RADIANTS?</td>\n",
       "      <td>Shroud</td>\n",
       "      <td>UCoz3Kpu5lv-ALhR4h9bDvcw</td>\n",
       "      <td>2022-10-14 17:10:13</td>\n",
       "      <td>10:16</td>\n",
       "      <td>222</td>\n",
       "      <td>10695</td>\n",
       "      <td>286814</td>\n",
       "      <td>_Ktomq5yor4</td>\n",
       "      <td>https://www.youtube.com/watch?v=_Ktomq5yor4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SO WE TRIED A NEW JETT AND NEON COMBO IN VALORANT</td>\n",
       "      <td>Shroud</td>\n",
       "      <td>UCoz3Kpu5lv-ALhR4h9bDvcw</td>\n",
       "      <td>2022-10-30 14:24:11</td>\n",
       "      <td>10:18</td>\n",
       "      <td>206</td>\n",
       "      <td>9422</td>\n",
       "      <td>270718</td>\n",
       "      <td>ELNs_hXu1qQ</td>\n",
       "      <td>https://www.youtube.com/watch?v=ELNs_hXu1qQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>IS HE ACTUALLY TROLLING ME? | VALORANT | Ft. L...</td>\n",
       "      <td>Sydeon</td>\n",
       "      <td>UCtTWOND3uyl4tVc_FarDmpw</td>\n",
       "      <td>2021-07-18 21:50:32</td>\n",
       "      <td>10:26</td>\n",
       "      <td>14</td>\n",
       "      <td>1299</td>\n",
       "      <td>13959</td>\n",
       "      <td>DF-3YZHB9iQ</td>\n",
       "      <td>https://www.youtube.com/watch?v=DF-3YZHB9iQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>HOW I WAS ALMOST ON LOVE ISLAND</td>\n",
       "      <td>Sydeon</td>\n",
       "      <td>UCtTWOND3uyl4tVc_FarDmpw</td>\n",
       "      <td>2022-11-02 19:00:04</td>\n",
       "      <td>45</td>\n",
       "      <td>106</td>\n",
       "      <td>16923</td>\n",
       "      <td>269679</td>\n",
       "      <td>BHZggI9E42o</td>\n",
       "      <td>https://www.youtube.com/watch?v=BHZggI9E42o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>SHE'S ACTUALLY TROLLING | VALORANT | Ft. Pokim...</td>\n",
       "      <td>Sydeon</td>\n",
       "      <td>UCtTWOND3uyl4tVc_FarDmpw</td>\n",
       "      <td>2021-05-31 21:15:15</td>\n",
       "      <td>13:21</td>\n",
       "      <td>51</td>\n",
       "      <td>2181</td>\n",
       "      <td>29033</td>\n",
       "      <td>jSvcWBehXTM</td>\n",
       "      <td>https://www.youtube.com/watch?v=jSvcWBehXTM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>THAT WAS FOR THE DOUBLE ACE | VALORANT | Ft. B...</td>\n",
       "      <td>Sydeon</td>\n",
       "      <td>UCtTWOND3uyl4tVc_FarDmpw</td>\n",
       "      <td>2021-05-17 20:23:02</td>\n",
       "      <td>16:6</td>\n",
       "      <td>51</td>\n",
       "      <td>1528</td>\n",
       "      <td>18337</td>\n",
       "      <td>S7ZfW1o2fRE</td>\n",
       "      <td>https://www.youtube.com/watch?v=S7ZfW1o2fRE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>HE'S ROASTING OUR TEAMATE! | VALORANT | Ft. Ed...</td>\n",
       "      <td>Sydeon</td>\n",
       "      <td>UCtTWOND3uyl4tVc_FarDmpw</td>\n",
       "      <td>2021-06-13 17:10:52</td>\n",
       "      <td>13:42</td>\n",
       "      <td>40</td>\n",
       "      <td>1981</td>\n",
       "      <td>25278</td>\n",
       "      <td>eXiYalbGQrI</td>\n",
       "      <td>https://www.youtube.com/watch?v=eXiYalbGQrI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>650 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title Channel Title  \\\n",
       "0                                             I'M BACK        Shroud   \n",
       "1    9 MINUTES OF SHROUD DESTROYING TRASH TALKERS I...        Shroud   \n",
       "2                               I have an announcement        Shroud   \n",
       "3                 CAN 4 VALORANT PROS BEAT 5 RADIANTS?        Shroud   \n",
       "4    SO WE TRIED A NEW JETT AND NEON COMBO IN VALORANT        Shroud   \n",
       "..                                                 ...           ...   \n",
       "645  IS HE ACTUALLY TROLLING ME? | VALORANT | Ft. L...        Sydeon   \n",
       "646                    HOW I WAS ALMOST ON LOVE ISLAND        Sydeon   \n",
       "647  SHE'S ACTUALLY TROLLING | VALORANT | Ft. Pokim...        Sydeon   \n",
       "648  THAT WAS FOR THE DOUBLE ACE | VALORANT | Ft. B...        Sydeon   \n",
       "649  HE'S ROASTING OUR TEAMATE! | VALORANT | Ft. Ed...        Sydeon   \n",
       "\n",
       "                   Channel ID         Publish time Duration  \\\n",
       "0    UCoz3Kpu5lv-ALhR4h9bDvcw  2022-11-29 22:16:54    10:14   \n",
       "1    UCoz3Kpu5lv-ALhR4h9bDvcw  2022-10-01 13:01:41     9:39   \n",
       "2    UCoz3Kpu5lv-ALhR4h9bDvcw  2022-11-23 22:19:00    10:14   \n",
       "3    UCoz3Kpu5lv-ALhR4h9bDvcw  2022-10-14 17:10:13    10:16   \n",
       "4    UCoz3Kpu5lv-ALhR4h9bDvcw  2022-10-30 14:24:11    10:18   \n",
       "..                        ...                  ...      ...   \n",
       "645  UCtTWOND3uyl4tVc_FarDmpw  2021-07-18 21:50:32    10:26   \n",
       "646  UCtTWOND3uyl4tVc_FarDmpw  2022-11-02 19:00:04       45   \n",
       "647  UCtTWOND3uyl4tVc_FarDmpw  2021-05-31 21:15:15    13:21   \n",
       "648  UCtTWOND3uyl4tVc_FarDmpw  2021-05-17 20:23:02     16:6   \n",
       "649  UCtTWOND3uyl4tVc_FarDmpw  2021-06-13 17:10:52    13:42   \n",
       "\n",
       "     Number of comments  Number of likes  Number of views     video_id  \\\n",
       "0                   418            10111           255065  jDW6uIbZHO0   \n",
       "1                   685            22175           561655  DTuS6Bki9kI   \n",
       "2                  1542            34935          1139897  7OWtsq-1V2Y   \n",
       "3                   222            10695           286814  _Ktomq5yor4   \n",
       "4                   206             9422           270718  ELNs_hXu1qQ   \n",
       "..                  ...              ...              ...          ...   \n",
       "645                  14             1299            13959  DF-3YZHB9iQ   \n",
       "646                 106            16923           269679  BHZggI9E42o   \n",
       "647                  51             2181            29033  jSvcWBehXTM   \n",
       "648                  51             1528            18337  S7ZfW1o2fRE   \n",
       "649                  40             1981            25278  eXiYalbGQrI   \n",
       "\n",
       "                                             url  \n",
       "0    https://www.youtube.com/watch?v=jDW6uIbZHO0  \n",
       "1    https://www.youtube.com/watch?v=DTuS6Bki9kI  \n",
       "2    https://www.youtube.com/watch?v=7OWtsq-1V2Y  \n",
       "3    https://www.youtube.com/watch?v=_Ktomq5yor4  \n",
       "4    https://www.youtube.com/watch?v=ELNs_hXu1qQ  \n",
       "..                                           ...  \n",
       "645  https://www.youtube.com/watch?v=DF-3YZHB9iQ  \n",
       "646  https://www.youtube.com/watch?v=BHZggI9E42o  \n",
       "647  https://www.youtube.com/watch?v=jSvcWBehXTM  \n",
       "648  https://www.youtube.com/watch?v=S7ZfW1o2fRE  \n",
       "649  https://www.youtube.com/watch?v=eXiYalbGQrI  \n",
       "\n",
       "[650 rows x 10 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime \n",
    "\n",
    "if os.path.exists(\"data/videos.csv\"):\n",
    "    # load any pre-existing data\n",
    "    df_videos = pd.read_csv('data/videos.csv')\n",
    "\n",
    "else:\n",
    "    videos_retrieved = []\n",
    "  \n",
    "    for channel_id in df[\"channel_id\"]:\n",
    "        #consumes 10k quota 100%\n",
    "        videos_retrieved.extend(get_videos_from_channel(youtube_service, channel_id,30))\n",
    "\n",
    "    df_videos = pd.DataFrame(videos_retrieved)\n",
    "    #save to csv file\n",
    "    df_videos.to_csv('data/videos.csv', index=False)\n",
    "    \n",
    "\n",
    "\n",
    "df_videos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217f062a",
   "metadata": {},
   "source": [
    "### Extracting Youtube comments from each video extracted \n",
    "Youtube API allows us to extract youtube comments, where we were able to extract all comments from each video. One key can provide us with 250,000 comments in one day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6ef1a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(youtube, **kwargs):\n",
    "    return youtube.commentThreads().list(\n",
    "        part=\"snippet\",\n",
    "        **kwargs\n",
    "    ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8673d507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments_video(videoId, total_comments = 5000, max_comment_per_page = 100 , order = \"time\"):\n",
    "    \n",
    "    comments_nb = 0 \n",
    "\n",
    "    list_comments = []\n",
    "    comments_dict = {}\n",
    "    \n",
    "    while comments_nb <total_comments:\n",
    "       \n",
    "        params = {\n",
    "                'videoId': videoId, \n",
    "                'maxResults': max_comment_per_page,\n",
    "                'order': 'relevance', # default is 'time' (newest)\n",
    "            }\n",
    "        try:\n",
    "            response = get_comments(youtube_service, **params)\n",
    "\n",
    "            items = response.get(\"items\")\n",
    "\n",
    "\n",
    "\n",
    "            # if items is empty, breakout of the loop\n",
    "            if not items:\n",
    "                break\n",
    "\n",
    "\n",
    "            for item in items:\n",
    "                if comments_nb == total_comments:\n",
    "                    break \n",
    "                comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "                comment_id = item['snippet']['topLevelComment']['id']\n",
    "                reply_count = item['snippet']['totalReplyCount']\n",
    "                like_count = item['snippet']['topLevelComment']['snippet']['likeCount']\n",
    "\n",
    "                comments_dict = {\n",
    "                    \"Comment ID\":comment_id, \n",
    "                    \"Comment\": comment,\n",
    "                    \"Likes\": like_count,\n",
    "                    \"Replies\": reply_count,\n",
    "                    \"Video ID\": videoId\n",
    "                    }\n",
    "                comments_nb+=1\n",
    "                list_comments.append(comments_dict)\n",
    "\n",
    "\n",
    "            if \"nextPageToken\" in response:\n",
    "                # if there is a next page\n",
    "                # add next page token to the params we pass to the function\n",
    "                params[\"pageToken\"] =  response[\"nextPageToken\"]\n",
    "            \n",
    "            else:\n",
    "                # must be end of comments!!!!\n",
    "                break\n",
    "                \n",
    "        except Exception:\n",
    "            if keys:          \n",
    "                print(\"switching keys\", len(list_comments))\n",
    "                get_service()\n",
    "                continue\n",
    "            else: \n",
    "                print(\"break\",len(list_comments) )\n",
    "                return list_comments\n",
    "\n",
    "\n",
    "    return list_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde36c2c",
   "metadata": {},
   "source": [
    "Saving/loading comment information into/from csv file after fetching 5 columns: Comment ID,\tComment,\tLikes,\tReplies,\tVideo ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0a2ce8c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Valkyrae: How are they losing to inebriated people?\\r<br>\\r<br>Abe: What do you mean we&#39;re winning? We lost the first six rounds.\\r<br>\\r<br>Sykunno: How much did she drink? Jesus...'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "if os.path.exists(\"data/comments.csv\"):\n",
    "    # load any pre-existing data\n",
    "    df_comments = pd.read_csv('data/comments.csv')\n",
    "else:\n",
    "    comments = []\n",
    "\n",
    "    for i , video_id in enumerate(df_videos[\"video_id\"]):\n",
    "        total_comments = int(df_videos[\"Number of comments\"][i])\n",
    "        if total_comments > 10000:\n",
    "            print(\"more than 10k\",total_comments)\n",
    "            total_comments = 10000  \n",
    "        print(\"next\")\n",
    "        comments.extend(get_comments_video(video_id, total_comments ) )\n",
    "\n",
    "    df_comments = pd.DataFrame(comments)\n",
    "    df_comments.to_csv('data/comments.csv', index=False)\n",
    "     \n",
    "\n",
    "df_comments[\"Comment\"].iloc[25729]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "57f5c3be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment ID</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Replies</th>\n",
       "      <th>Video ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UgyEWj-AR_EA35YdrFR4AaABAg</td>\n",
       "      <td>&lt;a href=\"about:invalid#zCSafez\"&gt;&lt;/a&gt; says if y...</td>\n",
       "      <td>143</td>\n",
       "      <td>9</td>\n",
       "      <td>jDW6uIbZHO0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UgzWmYT4Yfcjab-TSLZ4AaABAg</td>\n",
       "      <td>No doubt if you even take a break you will alw...</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>jDW6uIbZHO0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UgxqLK5mGmPeL0ezyKp4AaABAg</td>\n",
       "      <td>I wanna see Shroud&amp;#39;s Curved wall plays</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>jDW6uIbZHO0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UgyOqzTmv6-8-f4q0854AaABAg</td>\n",
       "      <td>I play valorant because of you. I hope you can...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>jDW6uIbZHO0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UgxrtgG4uYVJFqXcbhh4AaABAg</td>\n",
       "      <td>From my experience when it comes to chat and c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>jDW6uIbZHO0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281475</th>\n",
       "      <td>Ugy7dWzsqIFOy42mpxx4AaABAg</td>\n",
       "      <td>Teammate &lt;b&gt;**&lt;/b&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>eXiYalbGQrI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281476</th>\n",
       "      <td>UgzlF9IQC8nParp3e4h4AaABAg</td>\n",
       "      <td>u so fking beautiful ,... zz</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>eXiYalbGQrI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281477</th>\n",
       "      <td>Ugy1CQpBmVsVHqBW50N4AaABAg</td>\n",
       "      <td>wait thats me</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>eXiYalbGQrI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281478</th>\n",
       "      <td>UgzH01QXGAz21z0jBXR4AaABAg</td>\n",
       "      <td>omg im very early, what do I do</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>eXiYalbGQrI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281479</th>\n",
       "      <td>UgxVkBuMcZnRG01ntaV4AaABAg</td>\n",
       "      <td>Im here!!</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>eXiYalbGQrI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>281480 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Comment ID  \\\n",
       "0       UgyEWj-AR_EA35YdrFR4AaABAg   \n",
       "1       UgzWmYT4Yfcjab-TSLZ4AaABAg   \n",
       "2       UgxqLK5mGmPeL0ezyKp4AaABAg   \n",
       "3       UgyOqzTmv6-8-f4q0854AaABAg   \n",
       "4       UgxrtgG4uYVJFqXcbhh4AaABAg   \n",
       "...                            ...   \n",
       "281475  Ugy7dWzsqIFOy42mpxx4AaABAg   \n",
       "281476  UgzlF9IQC8nParp3e4h4AaABAg   \n",
       "281477  Ugy1CQpBmVsVHqBW50N4AaABAg   \n",
       "281478  UgzH01QXGAz21z0jBXR4AaABAg   \n",
       "281479  UgxVkBuMcZnRG01ntaV4AaABAg   \n",
       "\n",
       "                                                  Comment  Likes  Replies  \\\n",
       "0       <a href=\"about:invalid#zCSafez\"></a> says if y...    143        9   \n",
       "1       No doubt if you even take a break you will alw...     15        1   \n",
       "2              I wanna see Shroud&#39;s Curved wall plays      1        0   \n",
       "3       I play valorant because of you. I hope you can...      0        0   \n",
       "4       From my experience when it comes to chat and c...      0        0   \n",
       "...                                                   ...    ...      ...   \n",
       "281475                                 Teammate <b>**</b>      0        0   \n",
       "281476                       u so fking beautiful ,... zz      1        0   \n",
       "281477                                      wait thats me      2        1   \n",
       "281478                    omg im very early, what do I do      0        0   \n",
       "281479                                          Im here!!      2        0   \n",
       "\n",
       "           Video ID  \n",
       "0       jDW6uIbZHO0  \n",
       "1       jDW6uIbZHO0  \n",
       "2       jDW6uIbZHO0  \n",
       "3       jDW6uIbZHO0  \n",
       "4       jDW6uIbZHO0  \n",
       "...             ...  \n",
       "281475  eXiYalbGQrI  \n",
       "281476  eXiYalbGQrI  \n",
       "281477  eXiYalbGQrI  \n",
       "281478  eXiYalbGQrI  \n",
       "281479  eXiYalbGQrI  \n",
       "\n",
       "[281480 rows x 5 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2140800",
   "metadata": {},
   "source": [
    "## Cleaning comments:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0032e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "#pip install html5lib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "def clean_html(df):\n",
    "    # removing html markup \n",
    "    # Decoding html entities such as &pound;\n",
    "    #remove all extra spaces\n",
    "    # remove single character comments\n",
    "    # replace \n",
    "    index = []\n",
    "    \n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        txt = df.at[i,\"Comment\"]\n",
    "        txt = txt.strip()\n",
    "       \n",
    "        if len(txt) <= 1:\n",
    "            index.append(i)\n",
    "            print(txt)\n",
    "        else:\n",
    "            txt = re.sub(\"<.*?>\" , \"\" , txt )   \n",
    "            # Decoding html entities to normal characters\n",
    "            txt =  BeautifulSoup(txt, 'html5lib').text\n",
    "            txt = re.sub(r'\\s+', ' ', txt)\n",
    "            df.at[i,\"Comment\"] = txt \n",
    "            if len(txt) <= 1:\n",
    "                index.append(i)\n",
    "        \n",
    "                \n",
    "    print(len(index))\n",
    "    df.drop(index, inplace=True)\n",
    "\n",
    "    return df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e363a0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💜\n",
      "💜\n",
      "💜\n",
      "💜\n",
      "1\n",
      "1\n",
      "w\n",
      "🙌\n",
      "🗿\n",
      "🥱\n",
      "👍\n",
      "W\n",
      "W\n",
      "7\n",
      "z\n",
      "z\n",
      "a\n",
      "L\n",
      "🔥\n",
      "🔥\n",
      "🔥\n",
      "❤\n",
      "❤\n",
      "❤\n",
      "✨\n",
      "✨\n",
      "✨\n",
      "❤\n",
      "w\n",
      "💖\n",
      "❤\n",
      "💖\n",
      "🤣\n",
      "🔥\n",
      "🔥\n",
      "💖\n",
      "🤣\n",
      "X\n",
      "❤\n",
      "😁\n",
      "❤\n",
      "😶\n",
      "❤\n",
      "a\n",
      "😮\n",
      "W\n",
      "E\n",
      "E\n",
      "1\n",
      "5\n",
      "❤\n",
      "E\n",
      "w\n",
      ".\n",
      "🍪\n",
      "E\n",
      "😂\n",
      "E\n",
      "W\n",
      "W\n",
      "🥒\n",
      "🧐\n",
      ".\n",
      "😘\n",
      "e\n",
      "❤\n",
      "1\n",
      "a\n",
      "❤\n",
      "❤\n",
      "A\n",
      "👀\n",
      "…\n",
      "😂\n",
      ".\n",
      "w\n"
     ]
    }
   ],
   "source": [
    "df_comments.at[281475, \"Comment\"]\n",
    "df_c = clean_html(df_comments.copy())\n",
    "df_c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53989c0",
   "metadata": {},
   "source": [
    "### Join tables \n",
    "In order to manipulate the comments and to get a clear understanding of each comment, we joined all tables using their ID column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53b2023",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_video_channel = pd.merge(df, df_videos, how = 'left', left_on = ['channel_id'], right_on =['Channel ID'] )\n",
    "df_video_channel.drop(columns='channel_id')\n",
    "df_video_channel\n",
    "\n",
    "df_video_comment_data = pd.merge(df_video_channel , df_comments, how = 'right',  right_on =['Video ID'] , left_on= ['video_id'])\n",
    "df_video_comment_data.drop(columns='video_id')\n",
    "df_video_comment_data\n",
    "\n",
    "\n",
    "df_video_comment_data.to_csv('data/comments_videos_channel_info.csv')\n",
    "df_video_comment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e0c959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df):\n",
    "    small_youtuber = df[int(df[\"channel_subscriber_count\"])< 500000]\n",
    "    big_youtuber = df[int(df[\"channel_subscriber_count\"])>=500000]\n",
    "    return small_youtuber, big_youtuber "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9441d9",
   "metadata": {},
   "source": [
    "Splitting the dataset to small and big youtubers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20076ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_youtubers, big_youtubers = split_data(df_video_comment_channel_data)\n",
    "small_youtubers, big_youtubers\n",
    "\n",
    "small_youtubers.to_csv('data/small_youtubers.csv')\n",
    "big_youtubers.to_csv('data/big_youtubers.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e05befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_video_comment_data.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce95fc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'Valkyrae: How are they losing to inebriated people?\\r<br>\\r<br>Abe: What do you mean we&#39;re winning? We lost the first six rounds.\\r<br>\\r<br>Sykunno: How much did she drink? Jesus...'\n",
    "c = re.sub(\"<.*?>\" , \"\" ,a )\n",
    "c = re.sub(\"\\r?\" , \"\" ,c )\n",
    "c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f95c770c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valkyrae: How are they losing to inebriated people?\n",
      "\n",
      "Abe: What do you mean we're winning? We lost the first six rounds.\n",
      "\n",
      "Sykunno: How much did she drink? Jesus...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Valkyrae: How are they losing to inebriated people? Abe: What do you mean we're winning? We lost the first six rounds. Sykunno: How much did she drink? Jesus...\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a = 'Valkyrae: How are they losing to inebriated people?\\r<br>\\r<br>Abe: What do you mean we&#39;re winning? We lost the first six rounds.\\r<br>\\r<br>Sykunno: How much did she drink? Jesus...'\n",
    "\n",
    "# html_decoded_string = BeautifulSoup(a, convertEntities=BeautifulSoup.HTML_ENTITIES)\n",
    "# html_decoded_string\n",
    "\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "c = re.sub(\"<.*?>\" , \"\" , a )\n",
    "c = BeautifulSoup(c).get_text()\n",
    "print(c)\n",
    "d = re.sub(r'\\s+', ' ', c)\n",
    "\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6b3755c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valkyrae: How are they losing to inebriated people?\n",
      "\n",
      "Abe: What do you mean we're winning? We lost the first six rounds.\n",
      "\n",
      "Sykunno: How much did she drink? Jesus...\n"
     ]
    }
   ],
   "source": [
    "htmlDoc = BeautifulSoup(a, 'html5lib').text\n",
    "print(htmlDoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05c93df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
