{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea9782b3",
   "metadata": {},
   "source": [
    "# DSCI 511: Data Acquisition and Pre-Processing <br> Term Project Phase 2: Valorant comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01540c5",
   "metadata": {},
   "source": [
    "## Group members \n",
    "- Group member \n",
    "    - Name: Amira Bendjama\n",
    "    - Email: ab4745@drexel.edu\n",
    "- Group member \n",
    "    - Name: Nicole Padilla \n",
    "    - Email: np858@drexel.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d500e8f3",
   "metadata": {},
   "source": [
    "# Data collection \n",
    "\n",
    "Initial data was gathered via the YouTube API which allows publicly available YouTube comments to be called by anyone who created an app with their Google account. [GeeksforGeeks](https://www.geeksforgeeks.org/how-to-extract-youtube-comments-using-youtube-api-python/) was used as a reference for the code written to run the API call. The code was modified and resulting data was loaded into a .csv file.\n",
    "There are 22 youtubers selected based on their subscription count. We considered big youtubers the ones that their channels' subscription count surpass 500k, and under 500k to 100k are considered small youtubers. The lower bound 100k for subscription is how much the channel must reach in order to be verified, and since brands look for verified channels, we considered that limit. \n",
    "The youtubers information are collected in csv file \"Youtubers.csv\" that contains 3 columns: channel name, subscription count, channel's URL. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f2c8c2",
   "metadata": {},
   "source": [
    "### Youtubers \n",
    "In order to start collecting the comments, we needed a dataset of youtubers. Our selection was based on articles from [Best Valorant Streamers](https://www.esportsbets.com/valorant/streamers/), [Valorant main page on youtube](https://www.youtube.com/channel/UCiMRGE8Sc6oxIGuu_JxFoHg/live), reddit posts about [Valorant favorite youtubers](https://www.reddit.com/r/VALORANT/comments/o29j7i/favourite_valorant_youtuber/), and [Valorant YouTuber to learn the basics ?\n",
    "](https://www.reddit.com/r/VALORANT/comments/vz5mjp/valorant_youtuber_to_learn_the_basics/).\n",
    "\n",
    "__Criteria for picking streamers__: \n",
    "- Only verified channels, with a lower bound of subscription count of 100k, since the latter is how much the channel must reach in order to be eligible to apply for verification, and companies and brands will only consider verified channel to promote their products, in our case games.\n",
    "- Most valorant streamers are based on twitch, so a popular twitch streamers doesn’t qualify as a popular youtuber, so we picked valorant youtubers that upload on their main youtube channel and have a certain subscription count. \n",
    "- The valorant youtubers are split into two categories: Big youtubers above 500k subscription count, and small youtubers are under and above 100k. \n",
    "- Youtbers are english speakers from around the world, so it is not based on location but language.\n",
    "- Youtube channels are mixed between channels with only valorant videos, and channels with variety of other content besides valorant. Mainly to see the comment section through different communities. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405ab2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def get_channels_names(file_path):\n",
    "    youtubers = pd.read_csv(file_path, sep = \",\", header = 0)\n",
    "    return youtubers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1064cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "youtubers = get_channels_names(\"data/Youtubers.csv\")\n",
    "youtubers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce051067",
   "metadata": {},
   "source": [
    "## Youtube API \n",
    "In this project, we used Youtube API to retrieve comments, and videos from channels. We mainly used [youtube guide](https://developers.google.com/youtube/v3/getting-started), and other [ressources](https://towardsdatascience.com/how-to-build-your-own-dataset-of-youtube-comments-39a1e57aade). \n",
    "In order to access the API, a project must be created in [Google Developer’s Console](https://console.cloud.google.com/apis/dashboard?project=caramel-logic-370101), where you will have to do two steps: \n",
    "* Enable Youtube API data API v3.\n",
    "* Create API key.\n",
    "\n",
    "__Quota__ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147c1b8b",
   "metadata": {},
   "source": [
    "## Part 1: Retreiving Valorant Youtube comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a74162",
   "metadata": {},
   "source": [
    "### Building Youtube service \n",
    "After setting up the youtube API, we must install libraries for Google API client for python. <br>\n",
    "There is a quota limitation set by google at 10,000 units per day. To tackle this limitation, we used 4 different API keys to be able to retrieve the amount of videos and comments we want.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54afe589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys(file_path):\n",
    "    with open('data/keys.txt' , \"r\") as f: \n",
    "        keys = f.read()\n",
    "    keys = keys.split(\"\\n\")\n",
    "    return keys\n",
    "\n",
    "keys = get_keys('data/keys.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af63f266",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade google-api-python-client\n",
    "from googleapiclient.discovery import build\n",
    "#building youtube service\n",
    "def youtube_build_service(KEY):\n",
    "    \n",
    "    YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "    YOUTUBE_API_VERSION = \"v3\"\n",
    "\n",
    "    return build(YOUTUBE_API_SERVICE_NAME,\n",
    "                 YOUTUBE_API_VERSION,\n",
    "                 developerKey=KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d01a229",
   "metadata": {},
   "source": [
    "Getting keys to build the youtube service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e395f366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#each time call the service pop keys\n",
    "def get_service():\n",
    "    global youtube_service \n",
    "    if keys:\n",
    "        youtube_service  = youtube_build_service(keys.pop())       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f95663",
   "metadata": {},
   "source": [
    "Call the service each time the quota ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc29b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call this function to build the service \n",
    "#and also to switch keys\n",
    "get_service()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc951d47",
   "metadata": {},
   "source": [
    "### Channel information \n",
    "Each youtube channel has a unique channel ID, that mostly can be found at the end of the URL. However, some of old URL main channels will have the unique channel ID where others channels will have the name of the channel instead in form of: https://www.youtube.com/@namechannel. To solve issue, BeautifulSoup and requests were used to fecth html page of each channel and getting the unique ID by finding \"externalId\" that has the channel ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20dbc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def get_channel_id(channel_url):\n",
    "    url =\"\" \n",
    "    #getting json\n",
    "    resp = requests.get(channel_url)\n",
    "    data = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    #finding \"externalId\" that has the channel id no matter what is link structure\n",
    "    data_s = str(data)\n",
    "    \n",
    "    search_url = re.search('\"externalId\":',data_s)\n",
    "    start, end = search_url.span()\n",
    "    #finding the url after the id, using index\n",
    "    for i in range(end , end+100):\n",
    "        if data_s[i] == \",\":\n",
    "            break\n",
    "        url += data_s[i]\n",
    "    url = url.split('\"')[1]\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da004cf5",
   "metadata": {},
   "source": [
    "Using API call, to get channels information, specifiying statistics, snippets, contentDetails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2f649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_details(youtube, **kwargs):\n",
    "    return youtube.channels().list(\n",
    "        part=\"statistics,snippet,contentDetails\",\n",
    "        **kwargs\n",
    "    ).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2582402",
   "metadata": {},
   "source": [
    "Fetching each channel detail by providing the URL, then extracting the information needed from the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55e0e31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_channels_details_info(youtubers, youtube_service):\n",
    "    dict_youtubers = {}\n",
    "    l_youtubers = []\n",
    "    for index in range(len(youtubers[\"url\"])):\n",
    "        # get the channel ID from the URL\n",
    "        channel_id= get_channel_id(youtubers[\"url\"].iloc[index])\n",
    "        # get the channel details\n",
    "        response = get_channel_details(youtube_service, id=channel_id)\n",
    "        snippet = response[\"items\"][0][\"snippet\"]\n",
    "        statistics = response[\"items\"][0][\"statistics\"]\n",
    "        dict_youtubers = {\n",
    "            \"channel_id\":channel_id,\n",
    "            \"channel_title\" : snippet[\"title\"],\n",
    "            \"channel_subscriber_count\" : statistics[\"subscriberCount\"],\n",
    "            \"channel_video_count\" : statistics[\"videoCount\"],\n",
    "            \"channel_view_count\"  : statistics[\"viewCount\"] \n",
    "        }\n",
    "        l_youtubers.append(dict_youtubers)\n",
    "        \n",
    "    return l_youtubers\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be106191",
   "metadata": {},
   "source": [
    "Saving channels information into csv file after fetching 5 columns:\n",
    "* \"channel_id\"\n",
    "* \"channel_title\"\n",
    "* \"channel_subscriber_count\"\n",
    "* \"channel_video_count\"\n",
    "* \"channel_view_count\" </br>\n",
    "\n",
    "Or loading the file from \"./data/channels_info.csv\". </br>\n",
    "All the information will be presented as dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbb76a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists(\"data/channels_info.csv\"):\n",
    "    # load any pre-existing data\n",
    "    df = pd.read_csv('data/channels_info.csv')\n",
    "    df.pop(df.columns[0])\n",
    "else:\n",
    "    channels_info = get_channels_details_info(youtubers, youtube_service)\n",
    "    df = pd.DataFrame(channels_info)\n",
    "    #save to csv file\n",
    "    df.to_csv('data/channels_info.csv', index=False)\n",
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dfb3c2",
   "metadata": {},
   "source": [
    "### Extracting videos from each channel\n",
    "Manually picking up valorant videos for each channel isn't convenient. In addition, most videos won't have valorant in the title.To address this issue, we used the [search()](https://developers.google.com/youtube/v3/docs/search/list) offered by youtube API, where it has \"q\" paramter that specifies the query term to search for.<br>\n",
    "We were able to extract roughly 462 videos,by fetching 21 videos for each youtuber."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d2b81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_videos(youtube, **kwargs):\n",
    "    return youtube.search().list(\n",
    "        **kwargs\n",
    "    ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b500a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_details(youtube, **kwargs):\n",
    "    return youtube.videos().list(\n",
    "        part=\"snippet,contentDetails,statistics\",\n",
    "        **kwargs\n",
    "    ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9339eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_infos(video_response):\n",
    "     \n",
    "    items = video_response.get(\"items\")[0]\n",
    "    # get the snippet, statistics & content details from the video response\n",
    "    snippet         = items[\"snippet\"]\n",
    "    statistics      = items[\"statistics\"]\n",
    "    content_details = items[\"contentDetails\"]\n",
    "    # get infos from the snippet\n",
    "    channel_title = snippet[\"channelTitle\"]\n",
    "    channel_id = snippet[\"channelId\"]\n",
    "    title         = snippet[\"title\"]\n",
    "    publish_time  = snippet[\"publishedAt\"]\n",
    "    \n",
    "    # get stats infos\n",
    "    comment_count = statistics[\"commentCount\"]\n",
    "    like_count    = statistics[\"likeCount\"]\n",
    "    view_count    = statistics[\"viewCount\"]\n",
    "    # get duration from content details\n",
    "    duration = content_details[\"duration\"]\n",
    "    \n",
    "    # duration in the form of something like 'PT5H50M15S'\n",
    "    # parsing it to be something like '5:50:15'\n",
    "    parsed_duration = re.search(f\"PT(\\d+H)?(\\d+M)?(\\d+S)?\", duration).groups()\n",
    "    duration_str = \"\"\n",
    "    for d in parsed_duration:\n",
    "        if d:\n",
    "            duration_str += f\"{d[:-1]}:\"\n",
    "    duration_str = duration_str.strip(\":\")\n",
    "    \n",
    "    dict_video_info = {\n",
    "        \"Title\": title,\n",
    "        \"Channel Title\": channel_title,\n",
    "        \"Channel ID\": channel_id,\n",
    "        \"Publish time\": publish_time,\n",
    "        \"Duration\": duration_str,\n",
    "        \"Number of comments\": comment_count,\n",
    "        \"Number of likes\": like_count,\n",
    "        \"Number of views\": view_count\n",
    "        \n",
    "    }\n",
    "    \n",
    "    return dict_video_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3da335",
   "metadata": {},
   "source": [
    "The main issue we faced throughout the project is the quota limitation. To handle that, we used try/except to handle the HttpError generated from reaching the limits. When our limits reached for a single key, it is switched to another key and we build the youtube service again. Also, we made sure to undestand the quota consumption for each function using [YouTube Data API v3 - Quota Calculator](https://developers.google.com/youtube/v3/determine_quota_cost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eddccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_videos_from_channel(youtube_service, channel_id, videos_limit = 5):\n",
    "    \n",
    "    # counting number of videos grabbed\n",
    "    n_videos = 0\n",
    "    next_page_token = None\n",
    "    list_videos = []\n",
    "    \n",
    "\n",
    "    while n_videos < videos_limit:\n",
    "        #paramters to select the videos\n",
    "        #only valorant related videos\n",
    "        params = {\n",
    "            'part': 'snippet',\n",
    "            'q': 'valorant',\n",
    "            'channelId': channel_id,\n",
    "            'type': 'video',\n",
    "        }\n",
    "        \n",
    "        if next_page_token:\n",
    "            params['pageToken'] = next_page_token\n",
    "        \n",
    "        try:\n",
    "            #getting channel videos based on parameters\n",
    "            res = get_channel_videos(youtube_service, **params)\n",
    "            #getting items\n",
    "            channel_videos = res.get(\"items\")\n",
    "\n",
    "            for video in channel_videos:\n",
    "                if n_videos == videos_limit:\n",
    "                    break\n",
    "\n",
    "                \n",
    "                video_id = video[\"id\"][\"videoId\"]\n",
    "                # easily construct video URL by its ID\n",
    "                video_url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "\n",
    "                video_response = get_video_details(youtube_service, id=video_id)\n",
    "\n",
    "                # get video details in dictionary\n",
    "                dictionary_video = video_infos(video_response)\n",
    "                dictionary_video[\"video_id\"] = video_id\n",
    "                dictionary_video[\"url\"] = video_url \n",
    "                #changed just location\n",
    "                n_videos += 1\n",
    "\n",
    "                list_videos.append(dictionary_video)\n",
    "\n",
    "            # if there is a next page, then add it to our parameters\n",
    "            # to proceed to the next page\n",
    "            if \"nextPageToken\" in res:\n",
    "                next_page_token = res[\"nextPageToken\"]\n",
    "        #catch the quota exception and switch keys\n",
    "        except Exception as e:\n",
    "            if keys:                 \n",
    "                print(\"switching keys\", len(list_videos))\n",
    "                get_service()\n",
    "                continue\n",
    "            else: \n",
    "                print(\"break\", len(list_videos))\n",
    "                return list_videos\n",
    "\n",
    "        \n",
    "    return list_videos\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebc4b331",
   "metadata": {},
   "source": [
    "Saving/loading channels information into/from csv file after fetching 10 columns: __Title,\tChannel Title, Channel ID, Publish time,\tDuration,\tNumber of comments,\tNumber of likes,\tNumber of views,\tvideo_id,\turl.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bba8ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if os.path.exists(\"data/videos_info.csv\"):\n",
    "    # load any pre-existing data\n",
    "    df_videos = pd.read_csv('data/videos_info.csv')\n",
    "    #dropping the index column\n",
    "    df_videos.pop(df_videos.columns[0])\n",
    "else:\n",
    "    videos_retrieved = []\n",
    "  \n",
    "    for channel_id in df[\"channel_id\"]:\n",
    "        videos_retrieved.extend(get_videos_from_channel(youtube_service, channel_id,21))\n",
    "        print(\"next video\")\n",
    "        print()\n",
    "\n",
    "    df_videos = pd.DataFrame(videos_retrieved)\n",
    "    #save to csv file\n",
    "    df_videos.to_csv('data/videos_info.csv', index=False)\n",
    "df_videos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92d95b37",
   "metadata": {},
   "source": [
    "### Extracting Youtube comments from each video extracted \n",
    "Youtube API allows us to extract youtube comments, where we were able to extract all comments from each video. One key can provide us with 250,000 comments in one day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ef1a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(youtube, **kwargs):\n",
    "    return youtube.commentThreads().list(\n",
    "        part=\"snippet\",\n",
    "        **kwargs\n",
    "    ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8673d507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments_video(videoId, total_comments = 10000, max_comment_per_page = 100 , order = \"time\"):\n",
    "    \n",
    "    comments_nb = 0 \n",
    "\n",
    "    list_comments = []\n",
    "    comments_dict = {}\n",
    "    \n",
    "    while comments_nb <total_comments:\n",
    "       \n",
    "        params = {\n",
    "                'videoId': videoId, \n",
    "                'maxResults': max_comment_per_page,\n",
    "                'order': 'relevance', # default is 'time' (newest)\n",
    "            }\n",
    "        try:\n",
    "            response = get_comments(youtube_service, **params)\n",
    "\n",
    "            items = response.get(\"items\")\n",
    "\n",
    "\n",
    "\n",
    "            # if items is empty, breakout of the loop\n",
    "            if not items:\n",
    "                break\n",
    "\n",
    "\n",
    "            for item in items:\n",
    "                if comments_nb == total_comments:\n",
    "                    break \n",
    "                comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "                comment_id = item['snippet']['topLevelComment']['id']\n",
    "                reply_count = item['snippet']['totalReplyCount']\n",
    "                like_count = item['snippet']['topLevelComment']['snippet']['likeCount']\n",
    "\n",
    "                comments_dict = {\n",
    "                    \"Comment ID\":comment_id, \n",
    "                    \"Comment\": comment,\n",
    "                    \"Likes\": like_count,\n",
    "                    \"Replies\": reply_count,\n",
    "                    \"Video ID\": videoId\n",
    "                    }\n",
    "                comments_nb+=1\n",
    "                list_comments.append(comments_dict)\n",
    "\n",
    "\n",
    "            if \"nextPageToken\" in response:\n",
    "                # if there is a next page\n",
    "                # add next page token to the params we pass to the function\n",
    "                params[\"pageToken\"] =  response[\"nextPageToken\"]\n",
    "            \n",
    "            else:\n",
    "                # must be end of comments!!!!\n",
    "                break\n",
    "                \n",
    "        except Exception:\n",
    "            if keys:          \n",
    "#                 print(\"switching keys\", len(list_comments))\n",
    "                get_service()\n",
    "                continue\n",
    "            else: \n",
    "#                 print(\"break\",len(list_comments) )\n",
    "                return list_comments\n",
    "\n",
    "\n",
    "    return list_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc21a404",
   "metadata": {},
   "source": [
    "Saving/loading comment information into/from csv file after fetching 5 columns: Comment ID,\tComment,\tLikes,\tReplies,\tVideo ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2ce8c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists(\"data/comments.csv\"):\n",
    "    # load any pre-existing data\n",
    "    df_comments = pd.read_csv('data/comments.csv')\n",
    "    df_comments.pop(df_comments.columns[0])\n",
    "else:\n",
    "    comments = []\n",
    "#     comments.extend(get_comments_video(\"DTuS6Bki9kI\", 684))\n",
    "    \n",
    "    for i , video_id in enumerate(df_videos[\"video_id\"]):\n",
    "#         print(\"next video\")\n",
    "        comments.extend(get_comments_video(video_id, df_videos[\"Number of comments\"][i]))\n",
    "\n",
    "    df_comments = pd.DataFrame(comments)\n",
    "    df_comments.to_csv('data/comments.csv', index=False)\n",
    "    \n",
    "\n",
    "df_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a675993",
   "metadata": {},
   "source": [
    "### Join tables \n",
    "In order to manipulate the comments and to get a clear understanding of each comment, we joined all tables using their ID column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30af0b50",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#join comment, video and channal data on video and channel id\n",
    "df_video_comment_data = pd.merge(df_videos, df_comments, how = 'outer', left_on = ['video_id'], right_on = ['Video ID'])\n",
    "df_video_comment_channel_data = pd.merge(df_video_comment_data, df_channel_info, how = 'outer', left_on = ['Channel ID'], right_on = ['channel_id'])\n",
    "\n",
    "df_video_comment_channel_data\n",
    "df_video_comment_channel_data.to_csv('data/comments_videos_channel_info.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
