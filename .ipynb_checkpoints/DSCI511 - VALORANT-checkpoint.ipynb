{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea9782b3",
   "metadata": {},
   "source": [
    "# DSCI 511: Data Acquisition and Pre-Processing <br> Term Project Phase 2: Valorant comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01540c5",
   "metadata": {},
   "source": [
    "## Group members \n",
    "- Group member \n",
    "    - Name: Amira Bendjama\n",
    "    - Email: ab4745@drexel.edu\n",
    "- Group member \n",
    "    - Name: Nicole Padilla \n",
    "    - Email: np858@drexel.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d500e8f3",
   "metadata": {},
   "source": [
    "# Data collection \n",
    "\n",
    "Initial data was gathered via the YouTube API which allows publicly available YouTube comments to be called by anyone who created an app with their Google account. [GeeksforGeeks](https://www.geeksforgeeks.org/how-to-extract-youtube-comments-using-youtube-api-python/) was used as a reference for the code written to run the API call. The code was modified and resulting data was loaded into a .csv file.\n",
    "There are 22 youtubers selected based on their subscription count. We considered big youtubers the ones that their channels' subscription count surpass 500k, and under 500k to 100k are considered small youtubers. The lower bound 100k for subscription is how much the channel must reach in order to be verified, and since brands look for verified channels, we considered that limit. \n",
    "The youtubers information are collected in csv file \"Youtubers.csv\" that contains 3 columns: channel name, subscription count, channel's URL. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f2c8c2",
   "metadata": {},
   "source": [
    "### Youtubers \n",
    "In order to start collecting the comments, we needed a dataset of youtubers. Our selection was based on articles from [Best Valorant Streamers](https://www.esportsbets.com/valorant/streamers/), [Valorant main page on youtube](https://www.youtube.com/channel/UCiMRGE8Sc6oxIGuu_JxFoHg/live), reddit posts about [Valorant favorite youtubers](https://www.reddit.com/r/VALORANT/comments/o29j7i/favourite_valorant_youtuber/), and [Valorant YouTuber to learn the basics ?\n",
    "](https://www.reddit.com/r/VALORANT/comments/vz5mjp/valorant_youtuber_to_learn_the_basics/).\n",
    "\n",
    "__Criteria for picking streamers__: \n",
    "- Only verified channels, with a lower bound of subscription count of 100k, since the latter is how much the channel must reach in order to be eligible to apply for verification, and companies and brands will only consider verified channel to promote their products, in our case games.\n",
    "- Most valorant streamers are based on twitch, so a popular twitch streamers doesnâ€™t qualify as a popular youtuber, so we picked valorant youtubers that upload on their main youtube channel and have a certain subscription count. \n",
    "- The valorant youtubers are split into two categories: Big youtubers above 500k subscription count, and small youtubers are under and above 100k. \n",
    "- Youtbers are english speakers from around the world, so it is not based on location but language.\n",
    "- Youtube channels are mixed between channels with only valorant videos, and channels with variety of other content besides valorant. Mainly to see the comment section through different communities. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "405ab2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def get_channels_names(file_path):\n",
    "    youtubers = pd.read_csv(file_path, sep = \",\", header = 0)\n",
    "    return youtubers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c1064cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_name</th>\n",
       "      <th>sub_count</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shroud</td>\n",
       "      <td>6.81M</td>\n",
       "      <td>https://www.youtube.com/@shroud/videos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sykkuno</td>\n",
       "      <td>2.89M</td>\n",
       "      <td>https://www.youtube.com/@Sykkuno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iiTzTimmy</td>\n",
       "      <td>1.63M</td>\n",
       "      <td>https://www.youtube.com/@iiTzTimmy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TenZ</td>\n",
       "      <td>1.59M</td>\n",
       "      <td>https://www.youtube.com/@TenZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Flights</td>\n",
       "      <td>918K</td>\n",
       "      <td>https://www.youtube.com/@Flightss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Grim</td>\n",
       "      <td>893K</td>\n",
       "      <td>https://www.youtube.com/c/GrimGuy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kaydae</td>\n",
       "      <td>879K</td>\n",
       "      <td>https://www.youtube.com/@Kyedae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fuslie</td>\n",
       "      <td>732K</td>\n",
       "      <td>https://www.youtube.com/@fuslie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tarik</td>\n",
       "      <td>660K</td>\n",
       "      <td>https://www.youtube.com/@tarik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MrLowlander</td>\n",
       "      <td>624K</td>\n",
       "      <td>https://www.youtube.com/@MrLowlander</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>noted</td>\n",
       "      <td>612K</td>\n",
       "      <td>https://www.youtube.com/@noted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Flexinja</td>\n",
       "      <td>474K</td>\n",
       "      <td>https://www.youtube.com/@Flexinja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>QuarterJade</td>\n",
       "      <td>434K</td>\n",
       "      <td>https://www.youtube.com/@QuarterJade/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>xirena</td>\n",
       "      <td>392K</td>\n",
       "      <td>https://www.youtube.com/@xirena</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Hiko</td>\n",
       "      <td>384K</td>\n",
       "      <td>https://www.youtube.com/@Hiko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>red</td>\n",
       "      <td>329K</td>\n",
       "      <td>https://www.youtube.com/@RedValorant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Keeoh</td>\n",
       "      <td>339K</td>\n",
       "      <td>https://www.youtube.com/@Keeoh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Ziptie</td>\n",
       "      <td>251K</td>\n",
       "      <td>https://www.youtube.com/@ZipTie/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>xChocoBars</td>\n",
       "      <td>243K</td>\n",
       "      <td>https://www.youtube.com/@xChocoBars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>vkimm</td>\n",
       "      <td>234K</td>\n",
       "      <td>https://www.youtube.com/@vkimm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Peak</td>\n",
       "      <td>172K</td>\n",
       "      <td>https://www.youtube.com/@GosuPeak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Sydeon</td>\n",
       "      <td>115K</td>\n",
       "      <td>https://www.youtube.com/@Sydeons</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    channel_name sub_count                                     url\n",
       "0         Shroud     6.81M  https://www.youtube.com/@shroud/videos\n",
       "1        Sykkuno     2.89M        https://www.youtube.com/@Sykkuno\n",
       "2     iiTzTimmy      1.63M      https://www.youtube.com/@iiTzTimmy\n",
       "3           TenZ     1.59M           https://www.youtube.com/@TenZ\n",
       "4       Flights       918K       https://www.youtube.com/@Flightss\n",
       "5           Grim     893K        https://www.youtube.com/c/GrimGuy\n",
       "6         Kaydae      879K         https://www.youtube.com/@Kyedae\n",
       "7        fuslie       732K         https://www.youtube.com/@fuslie\n",
       "8          Tarik      660K          https://www.youtube.com/@tarik\n",
       "9   MrLowlander       624K    https://www.youtube.com/@MrLowlander\n",
       "10        noted      612K           https://www.youtube.com/@noted\n",
       "11      Flexinja      474K       https://www.youtube.com/@Flexinja\n",
       "12  QuarterJade       434K   https://www.youtube.com/@QuarterJade/\n",
       "13       xirena      392K          https://www.youtube.com/@xirena\n",
       "14          Hiko      384K           https://www.youtube.com/@Hiko\n",
       "15           red      329K    https://www.youtube.com/@RedValorant\n",
       "16         Keeoh      339K          https://www.youtube.com/@Keeoh\n",
       "17       Ziptie       251K        https://www.youtube.com/@ZipTie/\n",
       "18    xChocoBars     243K      https://www.youtube.com/@xChocoBars\n",
       "19         vkimm      234K          https://www.youtube.com/@vkimm\n",
       "20         Peak       172K       https://www.youtube.com/@GosuPeak\n",
       "21        Sydeon      115K        https://www.youtube.com/@Sydeons"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtubers = get_channels_names(\"data/Youtubers.csv\")\n",
    "youtubers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce051067",
   "metadata": {},
   "source": [
    "## Youtube API \n",
    "In this project, we used Youtube API to retrieve comments, and videos from channels. We mainly used [youtube guide](https://developers.google.com/youtube/v3/getting-started), and other [ressources](https://towardsdatascience.com/how-to-build-your-own-dataset-of-youtube-comments-39a1e57aade). \n",
    "In order to access the API, a project must be created in [Google Developerâ€™s Console](https://console.cloud.google.com/apis/dashboard?project=caramel-logic-370101), where you will have to do two steps: \n",
    "* Enable Youtube API data API v3.\n",
    "* Create API key.\n",
    "\n",
    "__Quota__ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147c1b8b",
   "metadata": {},
   "source": [
    "## Part 1: Retreiving Valorant Youtube comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a74162",
   "metadata": {},
   "source": [
    "### Building Youtube service \n",
    "After setting up the youtube API, we must install libraries for Google API client for python. <br>\n",
    "There is a quota limitation set by google at 10,000 units per day. To tackle this limitation, we used 4 different API keys to be able to retrieve the amount of videos and comments we want.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54afe589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys(file_path):\n",
    "    with open('data/keys.txt' , \"r\") as f: \n",
    "        keys = f.read()\n",
    "    keys = keys.split(\"\\n\")\n",
    "    return keys\n",
    "\n",
    "keys = get_keys('data/keys.txt')\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af63f266",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade google-api-python-client\n",
    "from googleapiclient.discovery import build\n",
    "#building youtube service\n",
    "def youtube_build_service(KEY):\n",
    "    \n",
    "    YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "    YOUTUBE_API_VERSION = \"v3\"\n",
    "\n",
    "    return build(YOUTUBE_API_SERVICE_NAME,\n",
    "                 YOUTUBE_API_VERSION,\n",
    "                 developerKey=KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d01a229",
   "metadata": {},
   "source": [
    "Getting keys to build the youtube service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e395f366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#each time call the service pop keys\n",
    "def get_service():\n",
    "    global youtube_service \n",
    "    if keys:\n",
    "        youtube_service  = youtube_build_service(keys.pop())       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f95663",
   "metadata": {},
   "source": [
    "Call the service each time the quota ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fc29b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<googleapiclient.discovery.Resource at 0x25a15c4aac0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#call this function to build the service \n",
    "#and also to switch keys\n",
    "get_service()\n",
    "youtube_service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc951d47",
   "metadata": {},
   "source": [
    "### Channel information \n",
    "Each youtube channel has a unique channel ID, that mostly can be found at the end of the URL. However, some of old URL main channels will have the unique channel ID where others channels will have the name of the channel instead in form of: https://www.youtube.com/@namechannel. To solve issue, BeautifulSoup and requests were used to fecth html page of each channel and getting the unique ID by finding \"externalId\" that has the channel ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a20dbc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def get_channel_id(channel_url):\n",
    "    url =\"\" \n",
    "    #getting json\n",
    "    resp = requests.get(channel_url)\n",
    "    data = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    #finding \"externalId\" that has the channel id no matter what is link structure\n",
    "    data_s = str(data)\n",
    "    \n",
    "    search_url = re.search('\"externalId\":',data_s)\n",
    "    start, end = search_url.span()\n",
    "    #finding the url after the id, using index\n",
    "    for i in range(end , end+100):\n",
    "        if data_s[i] == \",\":\n",
    "            break\n",
    "        url += data_s[i]\n",
    "    url = url.split('\"')[1]\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742d128e",
   "metadata": {},
   "source": [
    "Using API call, to get channels information, specifiying statistics, snippets, contentDetails.Also, Quota consumption is 1 quota for each youtube list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc2f649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_details(youtube, **kwargs):\n",
    "    return youtube.channels().list(\n",
    "        part=\"statistics,snippet,contentDetails\",\n",
    "        **kwargs\n",
    "    ).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2582402",
   "metadata": {},
   "source": [
    "Fetching each channel detail by providing the URL, then extracting the information needed from the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c55e0e31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_channels_details_info(youtubers, youtube_service):\n",
    "    dict_youtubers = {}\n",
    "    l_youtubers = []\n",
    "    for index in range(len(youtubers[\"url\"])):\n",
    "        # get the channel ID from the URL\n",
    "        channel_id= get_channel_id(youtubers[\"url\"].iloc[index])\n",
    "        # get the channel details\n",
    "        response = get_channel_details(youtube_service, id=channel_id)\n",
    "        snippet = response[\"items\"][0][\"snippet\"]\n",
    "        statistics = response[\"items\"][0][\"statistics\"]\n",
    "        dict_youtubers = {\n",
    "            \"channel_id\":channel_id,\n",
    "            \"channel_title\" : snippet[\"title\"],\n",
    "            \"channel_subscriber_count\" : statistics[\"subscriberCount\"],\n",
    "            \"channel_video_count\" : statistics[\"videoCount\"],\n",
    "            \"channel_view_count\"  : statistics[\"viewCount\"] \n",
    "        }\n",
    "        l_youtubers.append(dict_youtubers)\n",
    "        \n",
    "    return l_youtubers\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daf4b9d",
   "metadata": {},
   "source": [
    "Saving/loading channels information into/from \"./data/channels.csv\" after fetching 5 columns:\n",
    "* \"channel_id\"\n",
    "* \"channel_title\"\n",
    "* \"channel_subscriber_count\"\n",
    "* \"channel_video_count\"\n",
    "* \"channel_view_count\" </br>\n",
    "\n",
    "All the information will be presented as dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fbb76a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_id</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>channel_subscriber_count</th>\n",
       "      <th>channel_video_count</th>\n",
       "      <th>channel_view_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UCoz3Kpu5lv-ALhR4h9bDvcw</td>\n",
       "      <td>Shroud</td>\n",
       "      <td>6810000</td>\n",
       "      <td>1428</td>\n",
       "      <td>1007951954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UCRAEUAmW9kletIzOxhpLRFw</td>\n",
       "      <td>Sykkuno</td>\n",
       "      <td>2890000</td>\n",
       "      <td>641</td>\n",
       "      <td>371445453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UC5v2QgY2D5tlu8uws23MG4Q</td>\n",
       "      <td>iiTzTimmy</td>\n",
       "      <td>1630000</td>\n",
       "      <td>745</td>\n",
       "      <td>270690657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UCckPYr9b_iVucz8ID1Q67sw</td>\n",
       "      <td>TenZ</td>\n",
       "      <td>1590000</td>\n",
       "      <td>251</td>\n",
       "      <td>156859008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UCIfAlCwj-ZPZq5fqjpYDX3w</td>\n",
       "      <td>Flights</td>\n",
       "      <td>918000</td>\n",
       "      <td>56</td>\n",
       "      <td>96612905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UCWphjEePrzIrRA5mwcOt_4Q</td>\n",
       "      <td>Grim</td>\n",
       "      <td>893000</td>\n",
       "      <td>226</td>\n",
       "      <td>107176110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UCxjdy5n9BxX_6RTL8Dt_7pg</td>\n",
       "      <td>Kyedae</td>\n",
       "      <td>880000</td>\n",
       "      <td>81</td>\n",
       "      <td>52712496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>UCujyjxsq5FZNVnQro51zKSQ</td>\n",
       "      <td>fuslie</td>\n",
       "      <td>735000</td>\n",
       "      <td>785</td>\n",
       "      <td>120281865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>UCTbtlMEiBfs0zZLQyJzR0Uw</td>\n",
       "      <td>tarik</td>\n",
       "      <td>661000</td>\n",
       "      <td>1269</td>\n",
       "      <td>160465751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>UCgtbMb3djcXKj6CHerHwZ-A</td>\n",
       "      <td>MrLowlander</td>\n",
       "      <td>625000</td>\n",
       "      <td>366</td>\n",
       "      <td>182226931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>UCQ4dS_JStXcK3A30isduBbg</td>\n",
       "      <td>noted</td>\n",
       "      <td>613000</td>\n",
       "      <td>393</td>\n",
       "      <td>73104319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>UCqoJxH5s6xAiJ6QyevmuG7Q</td>\n",
       "      <td>Flexinja</td>\n",
       "      <td>474000</td>\n",
       "      <td>620</td>\n",
       "      <td>64033399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>UC_wSuaxwUYsJOBZDWwHIQZg</td>\n",
       "      <td>QuarterJade</td>\n",
       "      <td>435000</td>\n",
       "      <td>383</td>\n",
       "      <td>71545194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>UCdSad9tpJS8V8rL-4iuRuYw</td>\n",
       "      <td>xirena</td>\n",
       "      <td>393000</td>\n",
       "      <td>57</td>\n",
       "      <td>31534877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>UCRN1XC7PnnTL5R_GbYOMCZg</td>\n",
       "      <td>Hiko</td>\n",
       "      <td>384000</td>\n",
       "      <td>394</td>\n",
       "      <td>55728176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>UCFJ1pr8iwWPeQjmeHnPhqvA</td>\n",
       "      <td>Red</td>\n",
       "      <td>329000</td>\n",
       "      <td>294</td>\n",
       "      <td>53109590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>UCeIcwvxA_e5Dvrqg3rsuN1w</td>\n",
       "      <td>Keeoh</td>\n",
       "      <td>339000</td>\n",
       "      <td>384</td>\n",
       "      <td>69056269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>UCQ8VQZoYPeXF_q0E19UDGYQ</td>\n",
       "      <td>Ziptie</td>\n",
       "      <td>253000</td>\n",
       "      <td>238</td>\n",
       "      <td>81174305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>UCdH7fwkQ5RGVAMIAN2ufm4Q</td>\n",
       "      <td>xChocoBars</td>\n",
       "      <td>243000</td>\n",
       "      <td>570</td>\n",
       "      <td>18828574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>UCCBJqqk5h2hh8_WDGzrkRCQ</td>\n",
       "      <td>vkimm</td>\n",
       "      <td>235000</td>\n",
       "      <td>163</td>\n",
       "      <td>34621869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>UC-_1FH52GIOFGu4a8PzwRzQ</td>\n",
       "      <td>Peak</td>\n",
       "      <td>172000</td>\n",
       "      <td>486</td>\n",
       "      <td>23956005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>UCtTWOND3uyl4tVc_FarDmpw</td>\n",
       "      <td>Sydeon</td>\n",
       "      <td>115000</td>\n",
       "      <td>162</td>\n",
       "      <td>5914707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  channel_id channel_title  channel_subscriber_count  \\\n",
       "0   UCoz3Kpu5lv-ALhR4h9bDvcw        Shroud                   6810000   \n",
       "1   UCRAEUAmW9kletIzOxhpLRFw       Sykkuno                   2890000   \n",
       "2   UC5v2QgY2D5tlu8uws23MG4Q     iiTzTimmy                   1630000   \n",
       "3   UCckPYr9b_iVucz8ID1Q67sw          TenZ                   1590000   \n",
       "4   UCIfAlCwj-ZPZq5fqjpYDX3w       Flights                    918000   \n",
       "5   UCWphjEePrzIrRA5mwcOt_4Q          Grim                    893000   \n",
       "6   UCxjdy5n9BxX_6RTL8Dt_7pg        Kyedae                    880000   \n",
       "7   UCujyjxsq5FZNVnQro51zKSQ        fuslie                    735000   \n",
       "8   UCTbtlMEiBfs0zZLQyJzR0Uw         tarik                    661000   \n",
       "9   UCgtbMb3djcXKj6CHerHwZ-A   MrLowlander                    625000   \n",
       "10  UCQ4dS_JStXcK3A30isduBbg         noted                    613000   \n",
       "11  UCqoJxH5s6xAiJ6QyevmuG7Q      Flexinja                    474000   \n",
       "12  UC_wSuaxwUYsJOBZDWwHIQZg   QuarterJade                    435000   \n",
       "13  UCdSad9tpJS8V8rL-4iuRuYw        xirena                    393000   \n",
       "14  UCRN1XC7PnnTL5R_GbYOMCZg          Hiko                    384000   \n",
       "15  UCFJ1pr8iwWPeQjmeHnPhqvA           Red                    329000   \n",
       "16  UCeIcwvxA_e5Dvrqg3rsuN1w         Keeoh                    339000   \n",
       "17  UCQ8VQZoYPeXF_q0E19UDGYQ        Ziptie                    253000   \n",
       "18  UCdH7fwkQ5RGVAMIAN2ufm4Q    xChocoBars                    243000   \n",
       "19  UCCBJqqk5h2hh8_WDGzrkRCQ         vkimm                    235000   \n",
       "20  UC-_1FH52GIOFGu4a8PzwRzQ          Peak                    172000   \n",
       "21  UCtTWOND3uyl4tVc_FarDmpw        Sydeon                    115000   \n",
       "\n",
       "    channel_video_count  channel_view_count  \n",
       "0                  1428          1007951954  \n",
       "1                   641           371445453  \n",
       "2                   745           270690657  \n",
       "3                   251           156859008  \n",
       "4                    56            96612905  \n",
       "5                   226           107176110  \n",
       "6                    81            52712496  \n",
       "7                   785           120281865  \n",
       "8                  1269           160465751  \n",
       "9                   366           182226931  \n",
       "10                  393            73104319  \n",
       "11                  620            64033399  \n",
       "12                  383            71545194  \n",
       "13                   57            31534877  \n",
       "14                  394            55728176  \n",
       "15                  294            53109590  \n",
       "16                  384            69056269  \n",
       "17                  238            81174305  \n",
       "18                  570            18828574  \n",
       "19                  163            34621869  \n",
       "20                  486            23956005  \n",
       "21                  162             5914707  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists(\"data/channels.csv\"):\n",
    "    # load any pre-existing data\n",
    "    df = pd.read_csv('data/channels.csv')\n",
    "else:\n",
    "    channels_info = get_channels_details_info(youtubers, youtube_service)\n",
    "    df = pd.DataFrame(channels_info)\n",
    "    #save to csv file\n",
    "    df.to_csv('data/channels.csv', index=False)\n",
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ac8b9",
   "metadata": {},
   "source": [
    "### Extracting videos from each channel\n",
    "Manually picking up valorant videos for each channel isn't convenient. In addition, most videos won't have valorant in the title.To address this issue, we used the [search()](https://developers.google.com/youtube/v3/docs/search/list) offered by youtube API, where it has \"q\" paramter that specifies the query term to search for.<br>\n",
    "We were able to extract roughly 462 videos,by fetching 21 videos for each youtuber."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20d2b81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_videos(youtube, **kwargs):\n",
    "    return youtube.search().list(\n",
    "        **kwargs\n",
    "    ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b500a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_details(youtube, **kwargs):\n",
    "    return youtube.videos().list(\n",
    "        part=\"snippet,contentDetails,statistics\",\n",
    "        **kwargs\n",
    "    ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c9339eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_infos(video_response):\n",
    "     \n",
    "    items = video_response.get(\"items\")[0]\n",
    "    # get the snippet, statistics & content details from the video response\n",
    "    snippet         = items[\"snippet\"]\n",
    "    statistics      = items[\"statistics\"]\n",
    "    content_details = items[\"contentDetails\"]\n",
    "    # get infos from the snippet\n",
    "    channel_title = snippet[\"channelTitle\"]\n",
    "    channel_id = snippet[\"channelId\"]\n",
    "    title         = snippet[\"title\"]\n",
    "    publish_time  = snippet[\"publishedAt\"]\n",
    "    \n",
    "    # get stats infos\n",
    "    comment_count = statistics[\"commentCount\"]\n",
    "    like_count    = statistics[\"likeCount\"]\n",
    "    view_count    = statistics[\"viewCount\"]\n",
    "    # get duration from content details\n",
    "    duration = content_details[\"duration\"]\n",
    "    \n",
    "    # duration in the form of something like 'PT5H50M15S'\n",
    "    # parsing it to be something like '5:50:15'\n",
    "    parsed_duration = re.search(f\"PT(\\d+H)?(\\d+M)?(\\d+S)?\", duration).groups()\n",
    "    duration_str = \"\"\n",
    "    for d in parsed_duration:\n",
    "        if d:\n",
    "            duration_str += f\"{d[:-1]}:\"\n",
    "    duration_str = duration_str.strip(\":\")\n",
    "    \n",
    "    dict_video_info = {\n",
    "        \"Title\": title,\n",
    "        \"Channel Title\": channel_title,\n",
    "        \"Channel ID\": channel_id,\n",
    "        \"Publish time\": publish_time,\n",
    "        \"Duration\": duration_str,\n",
    "        \"Number of comments\": comment_count,\n",
    "        \"Number of likes\": like_count,\n",
    "        \"Number of views\": view_count\n",
    "        \n",
    "    }\n",
    "    \n",
    "    return dict_video_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3da335",
   "metadata": {},
   "source": [
    "The main issue we faced throughout the project is the quota limitation. To handle that, we used try/except to handle the HttpError generated from reaching the limits. When our limits reached for a single key, it is switched to another key and we build the youtube service again. Also, we made sure to undestand the quota consumption for each function using [YouTube Data API v3 - Quota Calculator](https://developers.google.com/youtube/v3/determine_quota_cost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9eddccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "def get_videos_from_channel(youtube_service, channel_id, videos_limit = 5):\n",
    "    \n",
    "    # counting number of videos grabbed\n",
    "    n_videos = 0\n",
    "    next_page_token = None\n",
    "    list_videos = []\n",
    "    \n",
    "\n",
    "    while n_videos < videos_limit:\n",
    "        #paramters to select the videos\n",
    "        #only valorant related videos\n",
    "        params = {\n",
    "            'part': 'snippet',\n",
    "            'q': 'valorant',\n",
    "            'channelId': channel_id,\n",
    "            'type': 'video',\n",
    "        }\n",
    "        \n",
    "        if next_page_token:\n",
    "            params['pageToken'] = next_page_token\n",
    "        \n",
    "        try:\n",
    "            #getting channel videos based on parameters\n",
    "            res = get_channel_videos(youtube_service, **params)\n",
    "            #getting items\n",
    "            channel_videos = res.get(\"items\")\n",
    "\n",
    "            for video in channel_videos:\n",
    "                if n_videos == videos_limit:\n",
    "                    break\n",
    "\n",
    "                \n",
    "                video_id = video[\"id\"][\"videoId\"]\n",
    "                # easily construct video URL by its ID\n",
    "                video_url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "\n",
    "                video_response = get_video_details(youtube_service, id=video_id)\n",
    "\n",
    "                # get video details in dictionary\n",
    "                dictionary_video = video_infos(video_response)\n",
    "                dictionary_video[\"video_id\"] = video_id\n",
    "                dictionary_video[\"url\"] = video_url \n",
    "                #changed just location\n",
    "                n_videos += 1\n",
    "\n",
    "                list_videos.append(dictionary_video)\n",
    "\n",
    "            # if there is a next page, then add it to our parameters\n",
    "            # to proceed to the next page\n",
    "            if \"nextPageToken\" in res:\n",
    "                next_page_token = res[\"nextPageToken\"]\n",
    "            \n",
    "            #sleep between requests\n",
    "            time.sleep(2)\n",
    "            \n",
    "        #catch the quota exception and switch keys\n",
    "        except Exception as e:\n",
    "            if keys:\n",
    "                #switch key and build service\n",
    "                get_service()\n",
    "                continue\n",
    "            else: \n",
    "                #in case of not having keys\n",
    "                return list_videos\n",
    "\n",
    "        \n",
    "    return list_videos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081ecb4a",
   "metadata": {},
   "source": [
    "Saving/loading channels information into/from csv file after fetching 10 columns: __Title,\tChannel Title, Channel ID, Publish time,\tDuration,\tNumber of comments,\tNumber of likes,\tNumber of views,\tvideo_id,\turl.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bba8ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel Title</th>\n",
       "      <th>Channel ID</th>\n",
       "      <th>Publish time</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Number of comments</th>\n",
       "      <th>Number of likes</th>\n",
       "      <th>Number of views</th>\n",
       "      <th>video_id</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shroud</td>\n",
       "      <td>UCoz3Kpu5lv-ALhR4h9bDvcw</td>\n",
       "      <td>2022-11-29T22:16:54Z</td>\n",
       "      <td>10:14</td>\n",
       "      <td>396</td>\n",
       "      <td>9215</td>\n",
       "      <td>221496</td>\n",
       "      <td>jDW6uIbZHO0</td>\n",
       "      <td>https://www.youtube.com/watch?v=jDW6uIbZHO0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shroud</td>\n",
       "      <td>UCoz3Kpu5lv-ALhR4h9bDvcw</td>\n",
       "      <td>2022-10-01T13:01:41Z</td>\n",
       "      <td>9:39</td>\n",
       "      <td>685</td>\n",
       "      <td>22141</td>\n",
       "      <td>560303</td>\n",
       "      <td>DTuS6Bki9kI</td>\n",
       "      <td>https://www.youtube.com/watch?v=DTuS6Bki9kI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shroud</td>\n",
       "      <td>UCoz3Kpu5lv-ALhR4h9bDvcw</td>\n",
       "      <td>2022-10-18T20:25:11Z</td>\n",
       "      <td>10:1</td>\n",
       "      <td>363</td>\n",
       "      <td>14583</td>\n",
       "      <td>435921</td>\n",
       "      <td>-Z2soOp0ZkQ</td>\n",
       "      <td>https://www.youtube.com/watch?v=-Z2soOp0ZkQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shroud</td>\n",
       "      <td>UCoz3Kpu5lv-ALhR4h9bDvcw</td>\n",
       "      <td>2022-10-30T14:24:11Z</td>\n",
       "      <td>10:18</td>\n",
       "      <td>206</td>\n",
       "      <td>9413</td>\n",
       "      <td>270325</td>\n",
       "      <td>ELNs_hXu1qQ</td>\n",
       "      <td>https://www.youtube.com/watch?v=ELNs_hXu1qQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shroud</td>\n",
       "      <td>UCoz3Kpu5lv-ALhR4h9bDvcw</td>\n",
       "      <td>2022-10-07T16:00:03Z</td>\n",
       "      <td>8:54</td>\n",
       "      <td>313</td>\n",
       "      <td>18793</td>\n",
       "      <td>584216</td>\n",
       "      <td>Cow9Qa9759Y</td>\n",
       "      <td>https://www.youtube.com/watch?v=Cow9Qa9759Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>Sydeon</td>\n",
       "      <td>UCtTWOND3uyl4tVc_FarDmpw</td>\n",
       "      <td>2021-07-18T21:50:32Z</td>\n",
       "      <td>10:26</td>\n",
       "      <td>14</td>\n",
       "      <td>1299</td>\n",
       "      <td>13959</td>\n",
       "      <td>DF-3YZHB9iQ</td>\n",
       "      <td>https://www.youtube.com/watch?v=DF-3YZHB9iQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>Sydeon</td>\n",
       "      <td>UCtTWOND3uyl4tVc_FarDmpw</td>\n",
       "      <td>2021-05-31T21:15:15Z</td>\n",
       "      <td>13:21</td>\n",
       "      <td>51</td>\n",
       "      <td>2181</td>\n",
       "      <td>29032</td>\n",
       "      <td>jSvcWBehXTM</td>\n",
       "      <td>https://www.youtube.com/watch?v=jSvcWBehXTM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>Sydeon</td>\n",
       "      <td>UCtTWOND3uyl4tVc_FarDmpw</td>\n",
       "      <td>2021-06-13T17:10:52Z</td>\n",
       "      <td>13:42</td>\n",
       "      <td>40</td>\n",
       "      <td>1981</td>\n",
       "      <td>25275</td>\n",
       "      <td>eXiYalbGQrI</td>\n",
       "      <td>https://www.youtube.com/watch?v=eXiYalbGQrI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>Sydeon</td>\n",
       "      <td>UCtTWOND3uyl4tVc_FarDmpw</td>\n",
       "      <td>2021-03-09T00:17:46Z</td>\n",
       "      <td>15:25</td>\n",
       "      <td>40</td>\n",
       "      <td>1625</td>\n",
       "      <td>18133</td>\n",
       "      <td>MB18fkSyWss</td>\n",
       "      <td>https://www.youtube.com/watch?v=MB18fkSyWss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>Sydeon</td>\n",
       "      <td>UCtTWOND3uyl4tVc_FarDmpw</td>\n",
       "      <td>2021-09-14T20:07:44Z</td>\n",
       "      <td>12:36</td>\n",
       "      <td>38</td>\n",
       "      <td>1397</td>\n",
       "      <td>14090</td>\n",
       "      <td>Zd5z-kFBeJE</td>\n",
       "      <td>https://www.youtube.com/watch?v=Zd5z-kFBeJE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>603 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Channel Title                Channel ID          Publish time Duration  \\\n",
       "0          Shroud  UCoz3Kpu5lv-ALhR4h9bDvcw  2022-11-29T22:16:54Z    10:14   \n",
       "1          Shroud  UCoz3Kpu5lv-ALhR4h9bDvcw  2022-10-01T13:01:41Z     9:39   \n",
       "2          Shroud  UCoz3Kpu5lv-ALhR4h9bDvcw  2022-10-18T20:25:11Z     10:1   \n",
       "3          Shroud  UCoz3Kpu5lv-ALhR4h9bDvcw  2022-10-30T14:24:11Z    10:18   \n",
       "4          Shroud  UCoz3Kpu5lv-ALhR4h9bDvcw  2022-10-07T16:00:03Z     8:54   \n",
       "..            ...                       ...                   ...      ...   \n",
       "598        Sydeon  UCtTWOND3uyl4tVc_FarDmpw  2021-07-18T21:50:32Z    10:26   \n",
       "599        Sydeon  UCtTWOND3uyl4tVc_FarDmpw  2021-05-31T21:15:15Z    13:21   \n",
       "600        Sydeon  UCtTWOND3uyl4tVc_FarDmpw  2021-06-13T17:10:52Z    13:42   \n",
       "601        Sydeon  UCtTWOND3uyl4tVc_FarDmpw  2021-03-09T00:17:46Z    15:25   \n",
       "602        Sydeon  UCtTWOND3uyl4tVc_FarDmpw  2021-09-14T20:07:44Z    12:36   \n",
       "\n",
       "     Number of comments  Number of likes  Number of views     video_id  \\\n",
       "0                   396             9215           221496  jDW6uIbZHO0   \n",
       "1                   685            22141           560303  DTuS6Bki9kI   \n",
       "2                   363            14583           435921  -Z2soOp0ZkQ   \n",
       "3                   206             9413           270325  ELNs_hXu1qQ   \n",
       "4                   313            18793           584216  Cow9Qa9759Y   \n",
       "..                  ...              ...              ...          ...   \n",
       "598                  14             1299            13959  DF-3YZHB9iQ   \n",
       "599                  51             2181            29032  jSvcWBehXTM   \n",
       "600                  40             1981            25275  eXiYalbGQrI   \n",
       "601                  40             1625            18133  MB18fkSyWss   \n",
       "602                  38             1397            14090  Zd5z-kFBeJE   \n",
       "\n",
       "                                             url  \n",
       "0    https://www.youtube.com/watch?v=jDW6uIbZHO0  \n",
       "1    https://www.youtube.com/watch?v=DTuS6Bki9kI  \n",
       "2    https://www.youtube.com/watch?v=-Z2soOp0ZkQ  \n",
       "3    https://www.youtube.com/watch?v=ELNs_hXu1qQ  \n",
       "4    https://www.youtube.com/watch?v=Cow9Qa9759Y  \n",
       "..                                           ...  \n",
       "598  https://www.youtube.com/watch?v=DF-3YZHB9iQ  \n",
       "599  https://www.youtube.com/watch?v=jSvcWBehXTM  \n",
       "600  https://www.youtube.com/watch?v=eXiYalbGQrI  \n",
       "601  https://www.youtube.com/watch?v=MB18fkSyWss  \n",
       "602  https://www.youtube.com/watch?v=Zd5z-kFBeJE  \n",
       "\n",
       "[603 rows x 9 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists(\"data/videos.csv\"):\n",
    "    # load any pre-existing data\n",
    "    df_videos = pd.read_csv('data/videos.csv')\n",
    "    #dropping the index column\n",
    "    df_videos.pop(df_videos.columns[0])\n",
    "else:\n",
    "    videos_retrieved = []\n",
    "  \n",
    "    for channel_id in df[\"channel_id\"]:\n",
    "        #make it 30\n",
    "        videos_retrieved.extend(get_videos_from_channel(youtube_service, channel_id,30))\n",
    "\n",
    "    df_videos = pd.DataFrame(videos_retrieved)\n",
    "    #save to csv file\n",
    "    df_videos.to_csv('data/videos.csv', index=False)\n",
    "df_videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217f062a",
   "metadata": {},
   "source": [
    "### Extracting Youtube comments from each video extracted \n",
    "Youtube API allows us to extract youtube comments, where we were able to extract all comments from each video. One key can provide us with 250,000 comments in one day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ef1a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(youtube, **kwargs):\n",
    "    return youtube.commentThreads().list(\n",
    "        part=\"snippet\",\n",
    "        **kwargs\n",
    "    ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8673d507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments_video(videoId, total_comments = 8000, max_comment_per_page = 100 , order = \"time\"):\n",
    "    \n",
    "    comments_nb = 0 \n",
    "\n",
    "    list_comments = []\n",
    "    comments_dict = {}\n",
    "    \n",
    "    while comments_nb <total_comments:\n",
    "       \n",
    "        params = {\n",
    "                'videoId': videoId, \n",
    "                'maxResults': max_comment_per_page,\n",
    "                'order': 'relevance', # default is 'time' (newest)\n",
    "            }\n",
    "        try:\n",
    "            response = get_comments(youtube_service, **params)\n",
    "\n",
    "            items = response.get(\"items\")\n",
    "\n",
    "\n",
    "\n",
    "            # if items is empty, breakout of the loop\n",
    "            if not items:\n",
    "                break\n",
    "\n",
    "\n",
    "            for item in items:\n",
    "                if comments_nb == total_comments:\n",
    "                    break \n",
    "                comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "                comment_id = item['snippet']['topLevelComment']['id']\n",
    "                reply_count = item['snippet']['totalReplyCount']\n",
    "                like_count = item['snippet']['topLevelComment']['snippet']['likeCount']\n",
    "\n",
    "                comments_dict = {\n",
    "                    \"Comment ID\":comment_id, \n",
    "                    \"Comment\": comment,\n",
    "                    \"Likes\": like_count,\n",
    "                    \"Replies\": reply_count,\n",
    "                    \"Video ID\": videoId\n",
    "                    }\n",
    "                comments_nb+=1\n",
    "                list_comments.append(comments_dict)\n",
    "\n",
    "\n",
    "            if \"nextPageToken\" in response:\n",
    "                # if there is a next page\n",
    "                # add next page token to the params we pass to the function\n",
    "                params[\"pageToken\"] =  response[\"nextPageToken\"]\n",
    "            \n",
    "            else:\n",
    "                # must be end of comments!!!!\n",
    "                break\n",
    "                \n",
    "        except Exception:\n",
    "            if keys:          \n",
    "                print(\"switching keys\", len(list_comments))\n",
    "                get_service()\n",
    "                continue\n",
    "            else: \n",
    "                print(\"break\",len(list_comments) )\n",
    "                return list_comments\n",
    "\n",
    "\n",
    "    return list_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde36c2c",
   "metadata": {},
   "source": [
    "Saving/loading comment information into/from csv file after fetching 5 columns: Comment ID,\tComment,\tLikes,\tReplies,\tVideo ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2ce8c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next video 0\n",
      "next video 1\n",
      "next video 2\n",
      "next video 3\n",
      "next video 4\n",
      "next video 5\n",
      "next video 6\n",
      "next video 7\n",
      "next video 8\n",
      "next video 9\n",
      "next video 10\n",
      "next video 11\n",
      "next video 12\n",
      "next video 13\n",
      "next video 14\n",
      "next video 15\n",
      "next video 16\n",
      "next video 17\n",
      "next video 18\n",
      "next video 19\n",
      "next video 20\n",
      "next video 21\n",
      "next video 22\n",
      "next video 23\n",
      "next video 24\n",
      "next video 25\n",
      "next video 26\n",
      "next video 27\n",
      "next video 28\n",
      "next video 29\n",
      "next video 30\n",
      "next video 31\n",
      "next video 32\n",
      "next video 33\n",
      "next video 34\n",
      "next video 35\n",
      "next video 36\n",
      "next video 37\n",
      "next video 38\n",
      "next video 39\n",
      "next video 40\n",
      "next video 41\n",
      "next video 42\n",
      "next video 43\n",
      "next video 44\n",
      "next video 45\n",
      "next video 46\n",
      "next video 47\n",
      "next video 48\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"data/comments.csv\"):\n",
    "    # load any pre-existing data\n",
    "    df_comments = pd.read_csv('data/comments.csv')\n",
    "    df_comments.pop(df_comments.columns[0])\n",
    "else:\n",
    "    comments = []\n",
    "    \n",
    "    for i , video_id in enumerate(df_videos[\"video_id\"]):\n",
    "        print(\"next video\", i)\n",
    "        comments.extend(get_comments_video(video_id))\n",
    "\n",
    "    df_comments = pd.DataFrame(comments)\n",
    "    df_comments.to_csv('data/comments.csv', index=False)\n",
    "     \n",
    "\n",
    "df_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53989c0",
   "metadata": {},
   "source": [
    "### Join tables \n",
    "In order to manipulate the comments and to get a clear understanding of each comment, we joined all tables using their ID column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30af0b50",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#join comment, video and channal data on video and channel id\n",
    "df_video_comment_data = pd.merge(df_videos, df_comments, how = 'outer', left_on = ['video_id'], right_on = ['Video ID'])\n",
    "df_video_comment_channel_data = pd.merge(df_video_comment_data, df_channel_info, how = 'outer', left_on = ['Channel ID'], right_on = ['channel_id'])\n",
    "\n",
    "df_video_comment_channel_data\n",
    "df_video_comment_channel_data.to_csv('data/comments_videos_channel_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a6dc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys =[\"AIzaSyAkKs_1ndolibMgBUR94PQi1MJoGGM6mU0\", \"AIzaSyAnhWSJGOoFQwdiB5DFnNeMPfrGMUpm04w\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892e82e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811f1884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
