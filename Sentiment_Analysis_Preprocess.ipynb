{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18b21f75-7e28-438e-8aa9-7f05b85ee332",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DSCI 511: Data Acquisition and Pre-Processing <br> Term Project Phase 2: Pre-processing Valorant Commments for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b4c1f9-f8f5-46d5-9c70-59efc30e4e0e",
   "metadata": {},
   "source": [
    "## Group members \n",
    "- Group member \n",
    "    - Name: Amira Bendjama\n",
    "    - Email: ab4745@drexel.edu\n",
    "- Group member \n",
    "    - Name: Nicole Padilla \n",
    "    - Email: np858@drexel.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1d1ac1-64a3-4aff-af39-b951d54930ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data pre-processing for sentiment analysis\n",
    "\n",
    "The goal of collecting the Valorant comment data from the YouTube API is to make it available for sentiment analysis. This sentiment analysis may be interesting to marketers, both of games and products targeted at gamers, streamers looking to improve their popularity or gaming companies seeking feedback.\n",
    "\n",
    "Before processing the data for analysis we offer the option to parse the data by multiple variables:  Channel ID or Video ID. For this final dataset we are using Channel ID.\n",
    "\n",
    "The following sources were referenced for determining the criteria and methods for data pre-processing for sentiment analysis:\n",
    "[Article 1](https://towardsdatascience.com/cleaning-preprocessing-text-data-for-sentiment-analysis-382a41f150d6), [Article 2](https://towardsdatascience.com/how-to-build-your-own-dataset-of-youtube-comments-39a1e57aade), [Article 3](https://neptune.ai/blog/tokenization-in-nlp).\n",
    "\n",
    "Based on these sources, and our own determinations, this pre-processing takes the following steps:\n",
    "1. Remove Emojis\n",
    "2. Strip URLs\n",
    "3. Clean up HTML text\n",
    "4. Convert all text to lower\n",
    "5. Handle contractions (replace contractions with full words, i.e. you're >> you are)\n",
    "6. Strip remaining extra characters\n",
    "7. Remove stop words\n",
    "8. Lemmatization (defined per Article 1 as \"Lemmatization removes the grammar tense and transforms each word into its original form\") \n",
    "9. Tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43cb99c-e8a1-4e2d-b279-b8385141b30e",
   "metadata": {},
   "source": [
    "### Grouping by YouTube IDs\n",
    "First, parse file containing comment, channel & video data (generated from YouTube API) for unique IDs. Unique IDs will be used to group comment data. For example, comment data grouped by channel ID would allow for sentiment analysis of an entire channel. \n",
    "\n",
    "Next, load a dictionary of with ID as key and list of comments as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "fe47c46f-51dd-4bce-967a-a00eddfadc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "#load unique ids from csv file based on either channel or video id\n",
    "def get_id_list(file_path, id_type = 'channel'):\n",
    "    \n",
    "    data = pd.read_csv(file_path, sep = \",\", header = 0)\n",
    "    \n",
    "    #set id_name based on id_type\n",
    "    id_name = ''\n",
    "    if id_type == 'channel':\n",
    "        id_name = 'channel_id'\n",
    "    if id_type == 'video':\n",
    "        id_name = 'video_id'\n",
    "        \n",
    "    #collect unique video IDs from file\n",
    "    id_list = []\n",
    "    \n",
    "    for unique_id in data[id_name]:\n",
    "        if unique_id not in id_list:\n",
    "            id_list.append(unique_id)\n",
    "            \n",
    "    return id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "e2a96f59-37a1-4543-a0e6-91bfa23324ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return channel IDs list\n",
    "channel_ids = get_id_list('data/comments_videos_channel_info.csv', 'channel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "409f6903-a52d-40a3-b918-5257c1703427",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load comments from file, grouped by unique id\n",
    "\n",
    "def get_comments_by_id(file_path, id_list, id_type = 'channel'):\n",
    "    \n",
    "    comments_by_id = {}\n",
    "    data = pd.read_csv(file_path, sep = \",\", header = 0) \n",
    "    \n",
    "    #set id_name based on id_type\n",
    "    id_name = ''\n",
    "    if id_type == 'channel':\n",
    "        id_name = 'Channel ID'\n",
    "    if id_type == 'video':\n",
    "        id_name = 'Video ID'\n",
    "        \n",
    "    for unique_id in id_list:\n",
    "        comments = []\n",
    "        for index, row in data.iterrows():\n",
    "            if row[id_name] == unique_id:\n",
    "                comments.append(row['Comment'])\n",
    "        comments_by_id[unique_id] = comments\n",
    "    \n",
    "    return comments_by_id\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "5d975ed6-f9ad-48be-8f5f-c81fa32df534",
   "metadata": {},
   "outputs": [],
   "source": [
    "#return comments by channel\n",
    "comments_by_channel = get_comments_by_id('data/comments_videos_channel_info.csv',channel_ids, 'channel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc0894c-8ea7-4d5d-83a3-1ea93a4ae2d9",
   "metadata": {},
   "source": [
    "### Cleaning the Comments\n",
    "\n",
    "Data cleaning is important for accurate sentiment analysis. If text contains large amounts of superflous words the sentiment analysis will not be accurate. \n",
    "\n",
    "Based on the unique aspects of this dataset (for example, it includes HTML markup) and researched best practices for data cleaning we are taking following steps to clean the text:\n",
    "\n",
    "1. Remove Emojis\n",
    "2. Strip URLs\n",
    "3. Clean up HTML text\n",
    "4. Convert all text to lower\n",
    "5. Handle contractions (replace contractions with full words, i.e. you're >> you are)\n",
    "6. Strip remaining extra characters\n",
    "7. Remove stop words\n",
    "8. Lemmatization (defined per Article 1 as \"Lemmatization removes the grammar tense and transforms each word into its original form\")\n",
    "9. Tokenization\n",
    "\n",
    "Refernce for why data cleaning is important: https://www.repustate.com/blog/data-cleaning-in-sentiment-analysis/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54227073-da7a-4e73-b20d-decef7e2fa37",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Remove emojis with demoji\n",
    "\n",
    "Demoji allows for the removal of emojis from text\n",
    "\n",
    "Demoji documentation can be found here: https://pypi.org/project/demoji/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f29678e-6ab1-4311-b5bf-633d94cf6ded",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Install demoji for identifying emojis\n",
    "#!pip install demoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4f7d454f-ea04-4a1b-bd2b-0b3b111390a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Emojis\n",
    "\n",
    "import demoji\n",
    "\n",
    "#Download codes (only once): demoji.download_codes()\n",
    "##https://pypi.org/project/demoji/\n",
    "\n",
    "#loop through video ids and clean URLs from each file\n",
    "def remove_emojis(text):\n",
    "    return demoji.replace(text, \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c106f5b8-e465-4e92-b9fc-17363163b47c",
   "metadata": {},
   "source": [
    "#### Remove URLS\n",
    "\n",
    "Comment data contains a mix of actual and broken links, all contained within HTML markup. Use the HTML markup to delineate links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d7a9b72c-545f-4196-b516-0ed2be509381",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove URLs\n",
    "# Use <a href to capture both actual URLs and broken links\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_urls(text):\n",
    "    return re.sub('<a href\\S+', ' ', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95590285-7406-4661-ae9d-3e7082387f11",
   "metadata": {},
   "source": [
    "#### Remove HTML chars\n",
    "\n",
    "This must be done before removing special characters in order to be accurate. Removes the break and quote HTML mark up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "19ec6add-7a55-4fc1-9e80-ef6c8c203cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove HTML chars ('quot', 'br') MUST DO BEFORE REMOVE SPECIAL CHARACTERS\n",
    "\n",
    "def remove_html(text):\n",
    "    remove_br = re.sub('<br>', ' ', text)\n",
    "    remove_quot = re.sub('&quot', ' ', remove_br)\n",
    "    return remove_quot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eecdff-1ddb-41e5-a337-d70ecb3ddf1c",
   "metadata": {},
   "source": [
    "#### Make all text lowercase\n",
    "\n",
    "When all text is the same case words will be recognized as the same (i.e. Word != word, but word == word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6b3e15cc-be8d-4b2b-99f3-7a1eaa1df946",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#make all text lowercase\n",
    "\n",
    "def text_lower(text):\n",
    "    new_text = [x.lower() for x in text.split()]\n",
    "    return ' '.join(new_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0629e16-b8eb-48ff-9999-c85c3ea40df6",
   "metadata": {},
   "source": [
    "### Handle contractions\n",
    "\n",
    "Using contractions library found here: https://www.geeksforgeeks.org/nlp-expand-contractions-in-text-processing/\n",
    "\n",
    "Replace contractions with the actual words, i.e. you're >> you are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6da330bb-38aa-4f0b-a69e-4e4c89ef0abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install contractions\n",
    "#!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "aa1424ee-a734-464c-b542-d4585b313152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handle contractions (i.e. #39)\n",
    "#YouTube comment data shows contractions as &#39;\n",
    "# import contractions\n",
    "import contractions\n",
    "\n",
    "def fix_contractions(text):\n",
    " \n",
    "    # create an empty list for expanded words\n",
    "    expanded_words = [] \n",
    "    \n",
    "    for word in text.split():\n",
    "        #sub contraction characters with apostrophe\n",
    "        clean_word = re.sub('&#39;', \"'\", word)\n",
    "        \n",
    "        # using contractions.fix to expand the shortened words\n",
    "        expanded_words.append(contractions.fix(clean_word))  \n",
    "\n",
    "    expanded_text = ' '.join(expanded_words)\n",
    "    return expanded_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2715fb95-01ca-4806-8da2-03ac90775535",
   "metadata": {},
   "source": [
    "#### Remove special characters\n",
    "\n",
    "Do this last. After all individual cases of text clean up have been performed, remove remaining special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "090a393e-d183-4c45-a7ab-79d240c1ee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove special characters\n",
    "import re\n",
    "\n",
    "def remove_special_char(text):\n",
    "    new_text = re.sub('[^A-Za-z0-9 ]+', '', text)\n",
    "    return new_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "ab7e43aa-b285-4404-be9c-940bc26c7a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean text of emojis and URLs\n",
    "\n",
    "def clean_text(comment_list):\n",
    "    \n",
    "    clean_comments = []\n",
    "    for comment in comment_list:\n",
    "        text = text_lower(remove_special_char(fix_contractions(remove_html(remove_emojis(remove_urls(comment))))))\n",
    "        clean_comments.append(text)\n",
    "                            \n",
    "    return clean_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "d3204b37-b3a2-4e77-95ef-9fc29a49af3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean text and return in dictionary format by ID\n",
    "\n",
    "def clean_text_dictionary(comments_dict, id_list):\n",
    "    clean_comments = {}\n",
    "    for unique_id in id_list:\n",
    "        clean_comments[unique_id] = clean_text(comments_dict[unique_id])\n",
    "    return clean_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "e0789092-7dbb-4dfa-bb79-7b7d8dd17bb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Clean comments dictionary of URLs and emojis\n",
    "\n",
    "comment_dict = clean_text_dictionary(comments_by_channel, channel_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b910e81-d10d-42b4-a70c-3d99f8db1410",
   "metadata": {},
   "source": [
    "### Remove stop words\n",
    "\n",
    "Use Natural Language Toolkit (nlkt) to define list of stopwords to remove\n",
    "\n",
    "https://www.nltk.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72060d6b-a3c1-48d7-95ee-cd9ffb24be95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download resources from Natural Language Toolkit (nltk)\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "011f12c0-eeea-4fdd-8170-bc3f7a3e0127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right tournament said team rae goes undefeated download valorant start playing know\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop = stopwords.words('english')\n",
    "    new_text = [x for x in text.split() if x not in stop]\n",
    "    return ' '.join(new_text)\n",
    "\n",
    "#print(remove_stopwords('so right before the tournament i said to myself if the team rae is on goes undefeated i will download valorant and start playing and what do you know'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0dce6b-0ad2-4a36-ab99-6ac778578836",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "Reference: https://pub.towardsai.net/how-and-why-to-implement-stemming-and-lemmatization-from-nltk-5f0cc69d2af\n",
    "\n",
    "Lemmatization resolves words to their dictionary form (for example, stripes becomes stripe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6f9c4793-2a89-44a9-bdab-9c48b9c4fcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_text = [lemmatizer.lemmatize(word) for word in text.split()]\n",
    "    return ' '.join(lemmatized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "e49b771d-fc8e-4f4c-b978-e7697ffc7b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopword and lemmatize clean_text\n",
    "\n",
    "def ready_to_tokenize(comment_list):\n",
    "    ready_to_tokenize = []\n",
    "    for comment in comment_list:   \n",
    "        ready_to_tokenize.append(lemmatize(remove_stopwords(comment)))\n",
    "    return ready_to_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "c39b7aca-1ea0-4065-8b18-5fd854cac85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare comment dictionary for tokenization by performing\n",
    "\n",
    "def ready_to_tokenize_dict(comment_dict, id_list):\n",
    "    dict_to_tokenize = {}\n",
    "    for unique_id in id_list:\n",
    "        dict_to_tokenize[unique_id] = ready_to_tokenize(comment_dict[unique_id])\n",
    "    return dict_to_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "5357bcc7-b3d9-49ee-9d75-92be4540851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return tokenization prepared dictionary from comment dictionary\n",
    "dict_to_tokenize = ready_to_tokenize_dict(comment_dict, channel_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b6d051-f8ce-4c09-972c-eff586ece2ad",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Because we have removed emojis and special characters prior to tokenization, simple whitespace tokenization is an effective method of tokenization for this dataset.\n",
    "\n",
    "Tokenization method was determined based on this study:\n",
    "http://sentiment.christopherpotts.net/tokenizing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b7787ece-ca33-4d10-8da4-af25ec535cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize text\n",
    "def tokenization(text):\n",
    "    return text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "eac07385-cf90-47e3-aa5d-0b20b0ec1ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_comments(comment_list):\n",
    "    tokenized_comments = []\n",
    "    for line in comment_list:\n",
    "        tokenized_comments.append(tokenization(line))\n",
    "    return tokenized_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "2ccef700-d344-48b2-ba6e-eab00c7ec05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize comment dictionary\n",
    "\n",
    "def tokenize_dict(comment_dict, id_list):\n",
    "    tokenized_dict = {}\n",
    "    for unique_id in id_list:\n",
    "        tokenized_dict[unique_id] = tokenize_comments(comment_dict[unique_id])\n",
    "    return tokenized_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "2ff37bb3-8e1c-48f6-9913-922876ae6157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return tokenized dictionary\n",
    "\n",
    "tokenized_dict = tokenize_dict(dict_to_tokenize, channel_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde0d9e0-5933-465c-967c-25398fc230fb",
   "metadata": {},
   "source": [
    "#### Save results in separate text files, by ID\n",
    "\n",
    "Files can then be fed into sentiment analysis API, results will give sentiment analysis of the specified group of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "f49dbec1-81a5-4d35-b3db-5cfcd237a9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sykkuno', 'UCRAEUAmW9kletIzOxhpLRFw'), ('iiTzTimmy', 'UC5v2QgY2D5tlu8uws23MG4Q'), ('Flights', 'UCIfAlCwj-ZPZq5fqjpYDX3w'), ('QuarterJade', 'UC_wSuaxwUYsJOBZDWwHIQZg'), ('Red', 'UCFJ1pr8iwWPeQjmeHnPhqvA'), ('Ziptie', 'UCQ8VQZoYPeXF_q0E19UDGYQ')]\n"
     ]
    }
   ],
   "source": [
    "#Get Channel Names to create clean file names\n",
    "\n",
    "file_path = 'data/comments_videos_channel_info.csv'\n",
    "\n",
    "data = pd.read_csv(file_path, sep = \",\", header = 0)\n",
    "\n",
    "channel_names = []\n",
    "for unique_id in channel_ids:\n",
    "    for index, row in data.iterrows():\n",
    "        if row['Channel ID'] == unique_id:\n",
    "            if (row['Channel Title'], unique_id) not in channel_names:\n",
    "                channel_names.append((row['Channel Title'], unique_id))\n",
    "print(channel_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "d02c92a4-883a-4cea-afdb-e7800eacd890",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results to .txt\n",
    "\n",
    "def save_files_by_channel(final_comment_dict, id_list):\n",
    "    \n",
    "    for channel_name in channel_names:\n",
    "        file_name = 'data/'+str(channel_name[0])+'.txt'\n",
    "        with open(file_name, 'w') as f:\n",
    "            for comment in tokenized_dict[unique_id]:\n",
    "                f.write(' '.join(comment))\n",
    "                f.write(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "bf7023d0-505a-4fe5-a37d-d51b8090b2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_files_by_channel(tokenized_dict, channel_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6512da-7827-44ce-8a53-1733ec6c0069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
