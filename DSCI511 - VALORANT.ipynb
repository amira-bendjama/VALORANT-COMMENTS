{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea9782b3",
   "metadata": {},
   "source": [
    "# DSCI 511: Data Acquisition and Pre-Processing <br> Term Project Phase 2: Valorant comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01540c5",
   "metadata": {},
   "source": [
    "## Group members \n",
    "- Group member \n",
    "    - Name: Amira Bendjama\n",
    "    - Email: ab4745@drexel.edu\n",
    "- Group member \n",
    "    - Name: Nicole Padilla \n",
    "    - Email: np858@drexel.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d500e8f3",
   "metadata": {},
   "source": [
    "# Data collection \n",
    "\n",
    "Initial data was gathered via the YouTube API which allows publicly available YouTube comments to be called by anyone who created an app with their Google account. [GeeksforGeeks](https://www.geeksforgeeks.org/how-to-extract-youtube-comments-using-youtube-api-python/) was used as a reference for the code written to run the API call. The code was modified and resulting data was loaded into a .csv file.\n",
    "There are 22 youtubers selected based on their subscription count. We considered big youtubers the ones that their channels' subscription count surpass 500k, and under 500k to 100k are considered small youtubers. The lower bound 100k for subscription is how much the channel must reach in order to be verified, and since brands look for verified channels, we considered that limit. \n",
    "The youtubers information are collected in csv file \"Youtubers.csv\" that contains 3 columns: channel name, subscription count, channel's URL. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f2c8c2",
   "metadata": {},
   "source": [
    "### Youtubers \n",
    "In order to start collecting the comments, we needed a dataset of youtubers. Our selection was based on articles from [Best Valorant Streamers](https://www.esportsbets.com/valorant/streamers/), [Valorant main page on youtube](https://www.youtube.com/channel/UCiMRGE8Sc6oxIGuu_JxFoHg/live), reddit posts about [Valorant favorite youtubers](https://www.reddit.com/r/VALORANT/comments/o29j7i/favourite_valorant_youtuber/), and [Valorant YouTuber to learn the basics ?\n",
    "](https://www.reddit.com/r/VALORANT/comments/vz5mjp/valorant_youtuber_to_learn_the_basics/).\n",
    "\n",
    "__Criteria for picking streamers__: \n",
    "- Only verified channels, with a lower bound of subscription count of 100k, since the latter is how much the channel must reach in order to be eligible to apply for verification, and companies and brands will only consider verified channel to promote their products, in our case games.\n",
    "- Most valorant streamers are based on twitch, so a popular twitch streamers doesnâ€™t qualify as a popular youtuber, so we picked valorant youtubers that upload on their main youtube channel and have a certain subscription count. \n",
    "- The valorant youtubers are split into two categories: Big youtubers above 500k subscription count, and small youtubers are under and above 100k. \n",
    "- Youtbers are english speakers from around the world, so it is not based on location but language.\n",
    "- Youtube channels are mixed between channels with only valorant videos, and channels with variety of other content besides valorant. Mainly to see the comment section through different communities. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "405ab2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def get_channels_names(file_path):\n",
    "    youtubers = pd.read_csv(file_path, sep = \",\", header = 0)\n",
    "    return youtubers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c1064cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_name</th>\n",
       "      <th>sub_count</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shroud</td>\n",
       "      <td>6.81M</td>\n",
       "      <td>https://www.youtube.com/@shroud/videos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sykkuno</td>\n",
       "      <td>2.89M</td>\n",
       "      <td>https://www.youtube.com/@Sykkuno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iiTzTimmy</td>\n",
       "      <td>1.63M</td>\n",
       "      <td>https://www.youtube.com/@iiTzTimmy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TenZ</td>\n",
       "      <td>1.59M</td>\n",
       "      <td>https://www.youtube.com/@TenZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Flights</td>\n",
       "      <td>918K</td>\n",
       "      <td>https://www.youtube.com/@Flightss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Grim</td>\n",
       "      <td>893K</td>\n",
       "      <td>https://www.youtube.com/c/GrimGuy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kaydae</td>\n",
       "      <td>879K</td>\n",
       "      <td>https://www.youtube.com/@Kyedae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fuslie</td>\n",
       "      <td>732K</td>\n",
       "      <td>https://www.youtube.com/@fuslie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tarik</td>\n",
       "      <td>660K</td>\n",
       "      <td>https://www.youtube.com/@tarik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MrLowlander</td>\n",
       "      <td>624K</td>\n",
       "      <td>https://www.youtube.com/@MrLowlander</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>noted</td>\n",
       "      <td>612K</td>\n",
       "      <td>https://www.youtube.com/@noted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Flexinja</td>\n",
       "      <td>474K</td>\n",
       "      <td>https://www.youtube.com/@Flexinja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>QuarterJade</td>\n",
       "      <td>434K</td>\n",
       "      <td>https://www.youtube.com/@QuarterJade/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>xirena</td>\n",
       "      <td>392K</td>\n",
       "      <td>https://www.youtube.com/@xirena</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Hiko</td>\n",
       "      <td>384K</td>\n",
       "      <td>https://www.youtube.com/@Hiko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>red</td>\n",
       "      <td>329K</td>\n",
       "      <td>https://www.youtube.com/@RedValorant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Keeoh</td>\n",
       "      <td>339K</td>\n",
       "      <td>https://www.youtube.com/@Keeoh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Ziptie</td>\n",
       "      <td>251K</td>\n",
       "      <td>https://www.youtube.com/@ZipTie/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>xChocoBars</td>\n",
       "      <td>243K</td>\n",
       "      <td>https://www.youtube.com/@xChocoBars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>vkimm</td>\n",
       "      <td>234K</td>\n",
       "      <td>https://www.youtube.com/@vkimm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Peak</td>\n",
       "      <td>172K</td>\n",
       "      <td>https://www.youtube.com/@GosuPeak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Sydeon</td>\n",
       "      <td>115K</td>\n",
       "      <td>https://www.youtube.com/@Sydeons</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    channel_name sub_count                                     url\n",
       "0         Shroud     6.81M  https://www.youtube.com/@shroud/videos\n",
       "1        Sykkuno     2.89M        https://www.youtube.com/@Sykkuno\n",
       "2     iiTzTimmy      1.63M      https://www.youtube.com/@iiTzTimmy\n",
       "3           TenZ     1.59M           https://www.youtube.com/@TenZ\n",
       "4       Flights       918K       https://www.youtube.com/@Flightss\n",
       "5           Grim     893K        https://www.youtube.com/c/GrimGuy\n",
       "6         Kaydae      879K         https://www.youtube.com/@Kyedae\n",
       "7        fuslie       732K         https://www.youtube.com/@fuslie\n",
       "8          Tarik      660K          https://www.youtube.com/@tarik\n",
       "9   MrLowlander       624K    https://www.youtube.com/@MrLowlander\n",
       "10        noted      612K           https://www.youtube.com/@noted\n",
       "11      Flexinja      474K       https://www.youtube.com/@Flexinja\n",
       "12  QuarterJade       434K   https://www.youtube.com/@QuarterJade/\n",
       "13       xirena      392K          https://www.youtube.com/@xirena\n",
       "14          Hiko      384K           https://www.youtube.com/@Hiko\n",
       "15           red      329K    https://www.youtube.com/@RedValorant\n",
       "16         Keeoh      339K          https://www.youtube.com/@Keeoh\n",
       "17       Ziptie       251K        https://www.youtube.com/@ZipTie/\n",
       "18    xChocoBars     243K      https://www.youtube.com/@xChocoBars\n",
       "19         vkimm      234K          https://www.youtube.com/@vkimm\n",
       "20         Peak       172K       https://www.youtube.com/@GosuPeak\n",
       "21        Sydeon      115K        https://www.youtube.com/@Sydeons"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtubers = get_channels_names(\"data/Youtubers.csv\")\n",
    "youtubers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce051067",
   "metadata": {},
   "source": [
    "## Youtube API \n",
    "In this project, we used Youtube API to retrieve comments, and videos from channels. We mainly used [youtube guide](https://developers.google.com/youtube/v3/getting-started), and other [ressources](https://towardsdatascience.com/how-to-build-your-own-dataset-of-youtube-comments-39a1e57aade). \n",
    "In order to access the API, a project must be created in [Google Developerâ€™s Console](https://console.cloud.google.com/apis/dashboard?project=caramel-logic-370101), where you will have to do two steps: \n",
    "* Enable Youtube API data API v3.\n",
    "* Create API key.\n",
    "\n",
    "__Quota__ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147c1b8b",
   "metadata": {},
   "source": [
    "## Part 1: Retreiving Valorant youtube videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a74162",
   "metadata": {},
   "source": [
    "### Building Youtube service \n",
    "After setting up the youtube API, we must install libraries for Google API client for python. <br>\n",
    "There is a quota limitation set by google at 10,000 units per day. To tackle this limitation, we used 4 different API keys to be able to retrieve the amount of videos and comments we want.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54afe589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys(file_path):\n",
    "    with open('data/keys.txt' , \"r\") as f: \n",
    "        keys = f.read()\n",
    "    keys = keys.split(\"\\n\")\n",
    "    return keys\n",
    "\n",
    "keys = get_keys('data/keys.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af63f266",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade google-api-python-client\n",
    "from googleapiclient.discovery import build\n",
    "#building youtube service\n",
    "def youtube_build_service(KEY):\n",
    "    \n",
    "    YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "    YOUTUBE_API_VERSION = \"v3\"\n",
    "\n",
    "    return build(YOUTUBE_API_SERVICE_NAME,\n",
    "                 YOUTUBE_API_VERSION,\n",
    "                 developerKey=KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d01a229",
   "metadata": {},
   "source": [
    "Getting keys to build the youtube service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e395f366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#each time call the service pop keys\n",
    "def get_service():\n",
    "    global youtube_service \n",
    "    if keys:\n",
    "        youtube_service  = youtube_build_service(keys.pop())       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f95663",
   "metadata": {},
   "source": [
    "Call the service each time the quota ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc29b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call this function to build the service \n",
    "#and also to switch keys\n",
    "get_service()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc951d47",
   "metadata": {},
   "source": [
    "Each youtube channel has a unique channel ID, that mostly can be found at the end of the URL in some of old URL main channels, but other channels will be on the form : https://www.youtube.com/@namechannel. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20dbc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def get_channel_id(channel_url):\n",
    "    url =\"\" \n",
    "    #getting json\n",
    "    resp = requests.get(channel_url)\n",
    "    data = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    #finding \"externalId\" that has the channel id no matter what is link structure\n",
    "    data_s = str(data)\n",
    "    \n",
    "    search_url = re.search('\"externalId\":',data_s)\n",
    "    start, end = search_url.span()\n",
    "    #finding the url after the id, using index\n",
    "    for i in range(end , end+100):\n",
    "        if data_s[i] == \",\":\n",
    "            break\n",
    "        url += data_s[i]\n",
    "    url = url.split('\"')[1]\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2f649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_details(youtube, **kwargs):\n",
    "    return youtube.channels().list(\n",
    "        part=\"statistics,snippet,contentDetails\",\n",
    "        **kwargs\n",
    "    ).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2582402",
   "metadata": {},
   "source": [
    "1 quota for each youtuber "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55e0e31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_channels_details_info(youtubers, youtube_service):\n",
    "    dict_youtubers = {}\n",
    "    l_youtubers = []\n",
    "    for index in range(len(youtubers[\"url\"])):\n",
    "        # get the channel ID from the URL\n",
    "        channel_id= get_channel_id(youtubers[\"url\"].iloc[index])\n",
    "        # get the channel details\n",
    "        response = get_channel_details(youtube_service, id=channel_id)\n",
    "        snippet = response[\"items\"][0][\"snippet\"]\n",
    "        statistics = response[\"items\"][0][\"statistics\"]\n",
    "        dict_youtubers = {\n",
    "            \"channel_id\":channel_id,\n",
    "            \"channel_title\" : snippet[\"title\"],\n",
    "            \"channel_subscriber_count\" : statistics[\"subscriberCount\"],\n",
    "            \"channel_video_count\" : statistics[\"videoCount\"],\n",
    "            \"channel_view_count\"  : statistics[\"viewCount\"] \n",
    "        }\n",
    "        l_youtubers.append(dict_youtubers)\n",
    "        \n",
    "    return l_youtubers\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbb76a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists(\"data/channels_info.csv\"):\n",
    "    # load any pre-existing data\n",
    "    df = pd.read_csv('data/channels_info.csv')\n",
    "    df.pop(df.columns[0])\n",
    "else:\n",
    "    channels_info = get_channels_details_info(youtubers, youtube_service)\n",
    "    df = pd.DataFrame(channels_info)\n",
    "    #save to csv file\n",
    "    df.to_csv('data/channels_info.csv', index=False)\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d2b81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_videos(youtube, **kwargs):\n",
    "    return youtube.search().list(\n",
    "        **kwargs\n",
    "    ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b500a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_details(youtube, **kwargs):\n",
    "    return youtube.videos().list(\n",
    "        part=\"snippet,contentDetails,statistics\",\n",
    "        **kwargs\n",
    "    ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9339eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_infos(video_response):\n",
    "     \n",
    "    items = video_response.get(\"items\")[0]\n",
    "    # get the snippet, statistics & content details from the video response\n",
    "    snippet         = items[\"snippet\"]\n",
    "    statistics      = items[\"statistics\"]\n",
    "    content_details = items[\"contentDetails\"]\n",
    "    # get infos from the snippet\n",
    "    channel_title = snippet[\"channelTitle\"]\n",
    "    channel_id = snippet[\"channelId\"]\n",
    "    title         = snippet[\"title\"]\n",
    "    publish_time  = snippet[\"publishedAt\"]\n",
    "    \n",
    "    # get stats infos\n",
    "    comment_count = statistics[\"commentCount\"]\n",
    "    like_count    = statistics[\"likeCount\"]\n",
    "    view_count    = statistics[\"viewCount\"]\n",
    "    # get duration from content details\n",
    "    duration = content_details[\"duration\"]\n",
    "    \n",
    "    # duration in the form of something like 'PT5H50M15S'\n",
    "    # parsing it to be something like '5:50:15'\n",
    "    parsed_duration = re.search(f\"PT(\\d+H)?(\\d+M)?(\\d+S)?\", duration).groups()\n",
    "    duration_str = \"\"\n",
    "    for d in parsed_duration:\n",
    "        if d:\n",
    "            duration_str += f\"{d[:-1]}:\"\n",
    "    duration_str = duration_str.strip(\":\")\n",
    "    \n",
    "    dict_video_info = {\n",
    "        \"Title\": title,\n",
    "        \"Channel Title\": channel_title,\n",
    "        \"Channel ID\": channel_id,\n",
    "        \"Publish time\": publish_time,\n",
    "        \"Duration\": duration_str,\n",
    "        \"Number of comments\": comment_count,\n",
    "        \"Number of likes\": like_count,\n",
    "        \"Number of views\": view_count\n",
    "        \n",
    "    }\n",
    "    \n",
    "    return dict_video_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3da335",
   "metadata": {},
   "source": [
    "\n",
    "quota 101 for 1 videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eddccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_videos_from_channel(youtube_service, channel_id, videos_limit = 5):\n",
    "    \n",
    "    # counting number of videos grabbed\n",
    "    n_videos = 0\n",
    "    next_page_token = None\n",
    "    list_videos = []\n",
    "    \n",
    "\n",
    "    while n_videos < videos_limit:\n",
    "        #paramters to select the videos\n",
    "        #only valorant related videos\n",
    "        params = {\n",
    "            'part': 'snippet',\n",
    "            'q': 'valorant',\n",
    "            'channelId': channel_id,\n",
    "            'type': 'video',\n",
    "        }\n",
    "        \n",
    "        if next_page_token:\n",
    "            params['pageToken'] = next_page_token\n",
    "        \n",
    "        try:\n",
    "            #getting channel videos based on parameters\n",
    "            res = get_channel_videos(youtube_service, **params)\n",
    "            #getting items\n",
    "            channel_videos = res.get(\"items\")\n",
    "\n",
    "            for video in channel_videos:\n",
    "                if n_videos == videos_limit:\n",
    "                    break\n",
    "\n",
    "                \n",
    "                video_id = video[\"id\"][\"videoId\"]\n",
    "                # easily construct video URL by its ID\n",
    "                video_url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "\n",
    "                video_response = get_video_details(youtube_service, id=video_id)\n",
    "\n",
    "                # get video details in dictionary\n",
    "                dictionary_video = video_infos(video_response)\n",
    "                dictionary_video[\"video_id\"] = video_id\n",
    "                dictionary_video[\"url\"] = video_url \n",
    "                #changed just location\n",
    "                n_videos += 1\n",
    "\n",
    "                list_videos.append(dictionary_video)\n",
    "\n",
    "            # if there is a next page, then add it to our parameters\n",
    "            # to proceed to the next page\n",
    "            if \"nextPageToken\" in res:\n",
    "                next_page_token = res[\"nextPageToken\"]\n",
    "        #catch the quota exception and switch keys\n",
    "        except Exception as e:\n",
    "            if keys:                 \n",
    "                print(\"switching keys\", len(list_videos))\n",
    "                get_service()\n",
    "                continue\n",
    "            else: \n",
    "                print(\"break\", len(list_videos))\n",
    "                return list_videos\n",
    "\n",
    "        \n",
    "    return list_videos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bba8ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if os.path.exists(\"data/videos_info.csv\"):\n",
    "    # load any pre-existing data\n",
    "    df_videos = pd.read_csv('data/videos_info.csv')\n",
    "    #dropping the index column\n",
    "    df_videos.pop(df_videos.columns[0])\n",
    "else:\n",
    "    videos_retrieved = []\n",
    "    #don't forget to remove the -2 index when you remove the two last small youtubers  \n",
    "    for channel_id in df[\"channel_id\"][:-2]:\n",
    "        videos_retrieved.extend(get_videos_from_channel(youtube_service, channel_id,21))\n",
    "        print(\"next video\")\n",
    "        print()\n",
    "\n",
    "    df_videos = pd.DataFrame(videos_retrieved)\n",
    "    #save to csv file\n",
    "    df_videos.to_csv('data/videos_info.csv', index=False)\n",
    "df_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f425e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting saved files\n",
    "df_videos = pd.read_csv('data/with id/videos_info_473_30comment.csv')\n",
    "df_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ef1a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(youtube, **kwargs):\n",
    "    return youtube.commentThreads().list(\n",
    "        part=\"snippet\",\n",
    "        **kwargs\n",
    "    ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8673d507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments_video(videoId, total_comments = 10000, max_comment_per_page = 100 , order = \"time\"):\n",
    "    \n",
    "    comments_nb = 0 \n",
    "\n",
    "    list_comments = []\n",
    "    comments_dict = {}\n",
    "    \n",
    "    while comments_nb <total_comments:\n",
    "       \n",
    "        params = {\n",
    "                'videoId': videoId, \n",
    "                'maxResults': max_comment_per_page,\n",
    "                'order': 'relevance', # default is 'time' (newest)\n",
    "            }\n",
    "        try:\n",
    "            response = get_comments(youtube_service, **params)\n",
    "\n",
    "            items = response.get(\"items\")\n",
    "\n",
    "\n",
    "\n",
    "            # if items is empty, breakout of the loop\n",
    "            if not items:\n",
    "                break\n",
    "\n",
    "\n",
    "            for item in items:\n",
    "                if comments_nb == total_comments:\n",
    "                    break \n",
    "                comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "                comment_id = item['snippet']['topLevelComment']['id']\n",
    "                reply_count = item['snippet']['totalReplyCount']\n",
    "                like_count = item['snippet']['topLevelComment']['snippet']['likeCount']\n",
    "\n",
    "                comments_dict = {\n",
    "                    \"Comment ID\":comment_id, \n",
    "                    \"Comment\": comment,\n",
    "                    \"Likes\": like_count,\n",
    "                    \"Replies\": reply_count,\n",
    "                    \"Video ID\": videoId\n",
    "                    }\n",
    "                comments_nb+=1\n",
    "                list_comments.append(comments_dict)\n",
    "\n",
    "\n",
    "            if \"nextPageToken\" in response:\n",
    "                # if there is a next page\n",
    "                # add next page token to the params we pass to the function\n",
    "                params[\"pageToken\"] =  response[\"nextPageToken\"]\n",
    "            \n",
    "            else:\n",
    "                # must be end of comments!!!!\n",
    "                break\n",
    "                \n",
    "        except Exception:\n",
    "            if keys:          \n",
    "#                 print(\"switching keys\", len(list_comments))\n",
    "                get_service()\n",
    "                continue\n",
    "            else: \n",
    "#                 print(\"break\",len(list_comments) )\n",
    "                return list_comments\n",
    "\n",
    "\n",
    "    return list_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2ce8c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists(\"data/comments.csv\"):\n",
    "    # load any pre-existing data\n",
    "    df_comments = pd.read_csv('data/comments.csv')\n",
    "    df_comments.pop(df_comments.columns[0])\n",
    "else:\n",
    "    comments = []\n",
    "#     comments.extend(get_comments_video(\"DTuS6Bki9kI\", 684))\n",
    "    \n",
    "    for i , video_id in enumerate(df_videos[\"video_id\"]):\n",
    "#         print(\"next video\")\n",
    "        comments.extend(get_comments_video(video_id, df_videos[\"Number of comments\"][i]))\n",
    "\n",
    "    df_comments = pd.DataFrame(comments)\n",
    "    df_comments.to_csv('data/comments.csv', index=False)\n",
    "    \n",
    "\n",
    "df_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30af0b50",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
